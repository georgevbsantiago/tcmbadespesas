---
title: "Web Scraping - Despesas dos Municípios do Estado da Bahia, via site do Tribunal de Contas dos Municípios do Estado da Bahia - TCM-Ba"
author: "George Santiago"
date: "18 de julho de 2018"
output: html_document
by: keepgrabbing.py - In memory of Aaron Swartz
---

# Sobre a proposta e objetivo do Web Scraping


# Sobre o Código do Web Scraping em Linguagem R


## Etapas e Estratégias do Web Scraping



## Estrutura do Código em Linguagem R do Web Scraping


### Código do Web Scraping em Linguagem R 
- Carregar os pacotes utilizados no script

Nessa etapa, iremos carregar os pacotes utilizadas em todo o Web Scraping, bem como definir o diretório de trabalho, que é de fundamental importância na etapa de criação das pastas para armazenamento dos dados obtidos. Se os pacotes não estivem instalados na máquina, será necessário utilizar o comando *install.packages("nome_do_pacote")* para que consiga ser carregado com na função *library()*


```{r setup, include=FALSE}

# Carregar pacotes utilizados no Web Scraping

# Pacote de Documentação do Web Scraping
if(require(rmarkdown) == FALSE) { install.packages("rmarkdown")}

#bPacotes que serão usados no processo de Web Scraping
if(require(httr) == FALSE) {install.packages("httr")}
if(require(rvest) == FALSE) {install.packages("rvest")}
if(require(xml2) == FALSE) {install.packages("xml2")}

# Pacote que será usado como ´timer´ para disparara a execução do Scraping em determinado dia e hora
#if(require(cronr) == FALSE) {install.packages("cronr")} - esse pacote ainda não está disponível para a a versão 3.5 do R

# Pacotes que serão usados na criação das tabela (data frames) e no durante o processo de tramento de dados (Data Wrangling)
if(require(tibble) == FALSE) { install.packages("tibble")}
if(require(readr) == FALSE) { install.packages("readr")}
if(require(dplyr) == FALSE) { install.packages("dplyr")}
if(require(tidyr) == FALSE) { install.packages("tidyr")}
if(require(stringr) == FALSE) { install.packages("stringr")}
if(require(stringi) == FALSE) { install.packages("stringi")}
if(require(lubridate) == FALSE) { install.packages("lubridate")}

# Pacotes que serão usados iterar (realizar os loops) das funções e paralelizar a execução do código
if(require(purrr) == FALSE) { install.packages("purrr")}
if(require(furrr) == FALSE) { install.packages("furrr")}

# Pacotes que serão usados para comunir com o Banco de Dados
if(require(DBI) == FALSE) { install.packages("DBI")}
if(require(RSQLite) == FALSE) { install.packages("RSQLite")}

# Pacotes que gera código Hash dos arquivos HTML baixados
if(require(git2r) == FALSE) { install.packages("git2r")}

# Pacote que será usado para disponibilizar os dados via API Rest 
if(require(plumber) == FALSE) { install.packages("plumber")}

# Pacotes que serão usados para construir, via Dockerfile, a arquitetura para execução do Web Scraping em "nuvem
if(require(packrat) == FALSE) { install.packages("packrat")}

# Pacotes que serão usados para comprimir arquivos para o Backup
if(require(zip) == FALSE) { install.packages("zip")}
if(require(googledrive) == FALSE) { install.packages("googledrive")}


```


01 - Bloco de código (chunk) que contém um conjunto de Funções variadas: Cria a data e hora local, com timezone do Brasil;...


```{r 01_funcoes_variadas}

# Função que cria a data e hora local, com timezone do Brasil
# Essa função foi desenvolvida para colocar a informação no formato DATE em formato 'character' no SQLite,
# visto que o SQLite converte data em número.
log_data_hora <- function () {

     format(lubridate::now(), tz ="Brazil/East", usetz = TRUE)

}

###################################################################

valor_monetario <- function (x) {
  
     readr::parse_number(x, locale = locale(grouping_mark = ".",
                                            decimal_mark = ","))

}

###################################################################

url_tcm <- function () {
  
url_tcm <-  "http://www.tcm.ba.gov.br/consulta-de-despesas/"
  
}

###################################################################

url_tcm_entidades_ws <- function(){

url_tcm_entidades_ws <- "http://www.tcm.ba.gov.br/Webservice/index.php/entidades?cdMunicipio="

}

###################################################################


```



02 - Bloco de código (chunk) que contém a Função que Criar o conjunto de pastas de trabalho (diretórios)


```{r 02_criar_diretorios}

criar_diretorios <- function() {

# Cria as pastas dos diretórios que serão utilizados;  

dir_principal <- getwd()

subdir_bd_sqlite <- file.path(dir_principal, "bd_sqlite")
subdir_resposta_scraping_html <- file.path(dir_principal, "resposta_scraping_html")
subdir_resposta_scraping_links <- file.path(dir_principal, "resposta_scraping_links")
subdir_dados_exportados <- file.path(dir_principal, "dados_exportados")
subdir_log_html <- file.path(dir_principal, "log_html")
subdir_backup <- file.path(dir_principal, "backup")

if (dir.exists(dir_principal) == FALSE) {
    dir.create(dir_principal)
}


if (dir.exists(subdir_bd_sqlite) == FALSE) {
    dir.create(subdir_bd_sqlite)
}

if (dir.exists(subdir_resposta_scraping_html) == FALSE) {
    dir.create(subdir_resposta_scraping_html)
}

if (dir.exists(subdir_resposta_scraping_links) == FALSE) {
    dir.create(subdir_resposta_scraping_links)
}

if (dir.exists(subdir_dados_exportados) == FALSE) {
    dir.create(subdir_dados_exportados)
}

if (dir.exists(subdir_log_html) == FALSE) {
    dir.create(subdir_log_html)
}

if (dir.exists(subdir_backup) == FALSE) {
    dir.create(subdir_backup)
}


print("As pastas foram criadas com sucesso no diretório")


}

```


03 - Bloco de código (chunk) que contém a Função que cria a conexão com o SGBD


```{r 03_connect_sgbd}


###################################################################

#!!! Criar 3 Banco de Dados. OLTP - Para o Web Scraping; Data Stage Area - Para os Dados Extraídos dos arquivos HMTL de Despesas; 4 - Data WareHouse - Para conectar ao Power BI


connect_sgbd <- function() {

# Verifica se o diretório onde ficará o arquivo do SQLite está criado;
if (dir.exists("bd_sqlite") == FALSE) { criar_diretorios() }

# Cria a conexão com o SQLite, assim como o arquivo 'bd_tcm_gastos_municipais.db', caso ele não exista;
sqlite_bd <- DBI::dbConnect(RSQLite::SQLite(), dbname = file.path("bd_sqlite", "bd_tcm_gastos_municipais.db"))

return(sqlite_bd)

}

###################################################################



```



04 - Bloco de código (chunk) que contém a Função que cria as tabelas no SGBD;


```{r 04_criar_tabelas_bd}


criar_tabelas_bd <- function() {
  
#conect_bd <- connect_sgbd()

if (DBI::dbExistsTable(connect_sgbd(), "tabela_dcalendario") == FALSE) {

    DBI::dbExecute(connect_sgbd(), "CREATE TABLE tabela_dcalendario (
                                                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                    data TEXT NOT NULL,
                                                    ano TEXT NOT NULL,
                                                    mes TEXT NOT NULL
                                                    );"
                                                    )
}



if (DBI::dbExistsTable(connect_sgbd(), "tabela_tcm_dmunicipios") == FALSE) {

    DBI::dbExecute(connect_sgbd(), "CREATE TABLE tabela_tcm_dmunicipios (
                                                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                    cod_municipio INT NOT NULL,
                                                    nm_municipio TEXT NOT NULL,
                                                    log_create TEXT NOT NULL
                                                    );"
                                                    )
}


if (DBI::dbExistsTable(connect_sgbd(), "tabela_tcm_dmunicipios_entidades") == FALSE) {

    DBI::dbExecute(connect_sgbd(), "CREATE TABLE tabela_tcm_dmunicipios_entidades (
                                                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                    cod_municipio INT NOT NULL,
                                                    nm_municipio TEXT NOT NULL,
                                                    cod_entidade INT NOT NULL,
                                                    nm_entidade TEXT NOT NULL,
                                                    log_create TEXT NOT NULL
                                                    );"
                                                    )
}
  

if (DBI::dbExistsTable(connect_sgbd(), "tabela_tcm_ddespesas") == FALSE) {

    DBI::dbExecute(connect_sgbd(), "CREATE TABLE tabela_tcm_ddespesas (
                                                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                    cod_municipio INT NOT NULL,
                                                    nm_municipio TEXT NOT NULL,
                                                    cod_entidade INT NOT NULL,
                                                    nm_entidade TEXT NOT NULL,
                                                    cod_despesa INT NOT NULL,
                                                    nm_despesa TEXT NOT NULL,
                                                    log_create TEXT NOT NULL
                                                    );"
                                                    )
}

  
if (DBI::dbExistsTable(connect_sgbd(), "tabela_tcm_dfontes_recursos") == FALSE) {

    DBI::dbExecute(connect_sgbd(), "CREATE TABLE tabela_tcm_dfontes_recursos (
                                                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                    cod_municipio INT NOT NULL,
                                                    nm_municipio TEXT NOT NULL,
                                                    cod_entidade INT NOT NULL,
                                                    nm_entidade TEXT NOT NULL,
                                                    cod_recurso INT NOT NULL,
                                                    nm_recurso TEXT NOT NULL,
                                                    log_create TEXT NOT NULL
                                                    );"
                                                    )
}


if (DBI::dbExistsTable(connect_sgbd(), "tabela_log") == FALSE) {

    DBI::dbExecute(connect_sgbd(), "CREATE TABLE tabela_log (
                                                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                  	log_erro TEXT NOT NULL,
                                                  	time TEXT NOT NULL,
                                                  	foreign_key	TEXT NOT NULL,
                                                  	nm_entidade	TEXT NOT NULL,
                                                  	pagina	TEXT NOT NULL,
                                                  	documento	TEXT NOT NULL,
                                                    arquivo_html TEXT NOT NULL,
                                                  	link TEXT NOT NULL
                                                    );"
                                                    )
}



if (DBI::dbExistsTable(connect_sgbd(), "tabela_entidades_alvos_paginas") == FALSE) {

    DBI::dbExecute(connect_sgbd(), "CREATE TABLE tabela_entidades_alvos_paginas (
                                                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                    ano TEXT NOT NULL,
                                                    cod_municipio INT NOT NULL,
                                                    nm_municipio TEXT NOT NULL,
                                                    cod_entidade INT NOT NULL,
                                                    nm_entidade TEXT NOT NULL,
                                                    pagina INT NOT NULL,
                                                    filtro INT NOT NULL
                                                    );"
                                                    )
}


if (DBI::dbExistsTable(connect_sgbd(), "tabela_paginas_links") == FALSE) {

    DBI::dbExecute(connect_sgbd(), "CREATE TABLE tabela_paginas_links (
                                                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                    ano TEXT NOT NULL,
                                                    cod_municipio INT NOT NULL,
                                                    nm_municipio TEXT NOT NULL,
                                                    cod_entidade INT NOT NULL,
                                                    nm_entidade TEXT NOT NULL,
                                                    pagina INT NOT NULL,
                                                    status_request_html_pag TEXT NOT NULL,
                                                    log_request_html_pag TEXT NOT NULL,
                                                    nm_arq_html_pag TEXT NOT NULL,
                                                    arq_html_pag_tratado TEXT NOT NULL,
                                                    hash_arq_html_pag TEXT NOT NULL,
                                                    log_tratamento_arq_html_pag TEXT NOT NULL
                                                    );"
                                                    )

}
  
if (DBI::dbExistsTable(connect_sgbd(), "tabela_requisicoes") == FALSE) {

    DBI::dbExecute(connect_sgbd(), "CREATE TABLE tabela_requisicoes (
                                                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                    ano TEXT NOT NULL,
                                                    cod_municipio INT NOT NULL,
                                                    nm_municipio TEXT NOT NULL,
                                                    cod_entidade INT NOT NULL,
                                                    nm_entidade TEXT NOT NULL,
                                                    pagina INT NOT NULL,
                                                    status_request_html_pag TEXT NOT NULL,
                                                    log_request_html_pag TEXT NOT NULL,
                                                    nm_arq_html_pag TEXT NOT NULL,
                                                    arq_html_pag_tratado TEXT NOT NULL,
                                                    hash_arq_html_pag TEXT NOT NULL,
                                                    log_tratamento_arq_html_pag TEXT NOT NULL,
                                                    documento TEXT NOT NULL,
                                                    empenho TEXT NOT NULL,
                                                    valor_documento TEXT NOT NULL,
                                                    link_despesa TEXT NOT NULL,
                                                    nm_arq_html_despesa TEXT NOT NULL,
                                                    status_request_html_despesa TEXT NOT NULL,
                                                    log_request_html_despesa TEXT NOT NULL,
                                                    arq_html_despesa_tratado TEXT NOT NULL,
                                                    hash_arq_html_despesa TEXT NOT NULL,
                                                    log_tratamento_arq_html_despesa TEXT NOT NULL
                                                    );"
                                                    )
  
}


 if (DBI::dbExistsTable(connect_sgbd(), "tabela_despesas_municipais") == FALSE) {

    DBI::dbExecute(connect_sgbd(), "CREATE TABLE tabela_despesas_municipais (
                                                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                    fase TEXT NOT NULL,
                                                    data_do_pagamento TEXT NOT NULL,
                                                    valor_do_pagamento TEXT NOT NULL,
                                                    documento TEXT NOT NULL,
                                                    empenho TEXT NOT NULL,
                                                    data_empenho TEXT NOT NULL,
                                                    tipo_de_empenho TEXT NOT NULL,
                                                    favorecido TEXT NOT NULL,
                                                    valor_do_empenho TEXT NOT NULL,
                                                    valor_das_retencoes TEXT NOT NULL,
                                                    restos_a_pagar TEXT NOT NULL,
                                                    conta_bancaria TEXT NOT NULL,
                                                    fonte_de_recurso_tcm TEXT NOT NULL,
                                                    fonte_de_recurso_gestor TEXT NOT NULL,
                                                    tipo_de_documento TEXT NOT NULL,
                                                    cod_municipio TEXT NOT NULL,
                                                    municipio TEXT NOT NULL,
                                                    cod_entidade TEXT NOT NULL,
                                                    nm_entidade TEXT NOT NULL,
                                                    poder TEXT NOT NULL,
                                                    orgao TEXT NOT NULL,
                                                    unidade_orcamentaria TEXT NOT NULL,
                                                    funcao TEXT NOT NULL,
                                                    subfuncao TEXT NOT NULL,
                                                    programa TEXT NOT NULL,
                                                    tipo_acao TEXT NOT NULL,
                                                    acao TEXT NOT NULL,
                                                    natureza_da_despesa_tcm TEXT NOT NULL,
                                                    natureza_da_despesa_gestor TEXT NOT NULL,
                                                    fonte_de_recurso_tcm_2 TEXT NOT NULL,
                                                    fonte_de_recurso_gestor_2 TEXT NOT NULL,
                                                    licitacao TEXT NOT NULL,
                                                    dispensa_inexigibilidade TEXT NOT NULL,
                                                    contrato TEXT NOT NULL,
                                                    declaracao TEXT NOT NULL,
                                                    foreign_key INTEGER NOT NULL,
                                                    nm_arq_html_despesa TEXT NOT NULL,
                                                    hash_arq_html_despesa TEXT NOT NULL,
                                                    log_tratamento_arq_html_despesa TEXT NOT NULL,
                                                    link TEXT NOT NULL
                                                    );"
                                                    )
  
}

if (DBI::dbExistsTable(connect_sgbd(), "tabela_despesas_municipais_tidy_data") == FALSE) {

    DBI::dbExecute(connect_sgbd(), "CREATE TABLE tabela_despesas_municipais_tidy_data (
                                                  	id INTEGER NOT NULL,
                                                  	fase TEXT NOT NULL,
                                                  	data_do_pagamento	TEXT NOT NULL,
                                                  	valor_do_pagamento	REAL NOT NULL,
                                                  	documento	TEXT NOT NULL,
                                                  	empenho	TEXT NOT NULL,
                                                  	data_empenho	TEXT NOT NULL,
                                                  	tipo_de_empenho	TEXT NOT NULL,
                                                  	cod_favorecido TEXT NOT NULL,
                                                  	nm_favorecido	TEXT NOT NULL,
                                                  	valor_do_empenho	REAL NOT NULL,
                                                  	valor_das_retencoes	REAL NOT NULL,
                                                  	restos_a_pagar TEXT NOT NULL,
                                                  	conta_bancaria TEXT NOT NULL,
                                                  	cod_fonte_de_recurso_tcm	TEXT NOT NULL,
                                                  	nm_fonte_de_recurso_tcm	TEXT NOT NULL,
                                                  	cod_fonte_de_recurso_gestor	TEXT NOT NULL,
                                                  	nm_fonte_de_recurso_gestor TEXT NOT NULL,
                                                  	tipo_de_documento	TEXT NOT NULL,
                                                  	cod_municipio	TEXT NOT NULL,
                                                  	municipio	TEXT NOT NULL,
                                                  	cod_entidade TEXT NOT NULL,
                                                  	nm_entidade	TEXT NOT NULL,
                                                  	poder	TEXT NOT NULL,
                                                  	cod_orgao	TEXT NOT NULL,
                                                  	nm_orgao TEXT NOT NULL,
                                                  	cod_unidade_orcamentaria	TEXT NOT NULL,
                                                  	nm_unidade_orcamentaria	TEXT NOT NULL,
                                                  	cod_funcao TEXT NOT NULL,
                                                  	nm_funcao	TEXT NOT NULL,
                                                  	cod_subfuncao TEXT NOT NULL,
                                                  	nm_subfuncao TEXT NOT NULL,
                                                  	cod_programa TEXT NOT NULL,
                                                  	nm_programa	TEXT NOT NULL,
                                                  	cod_tipo_acao TEXT NOT NULL,
                                                  	nm_tipo_acao TEXT NOT NULL,
                                                  	cod_acao TEXT NOT NULL,
                                                  	nm_acao	TEXT NOT NULL,
                                                  	cod_natureza_da_despesa_tcm TEXT NOT NULL,
                                                  	nm_natureza_da_despesa_tcm TEXT NOT NULL,
                                                  	cod_natureza_da_despesa_gestor TEXT NOT NULL,
                                                  	nm_natureza_da_despesa_gestor	TEXT NOT NULL,
                                                  	cod_fonte_de_recurso_tcm_2	TEXT NOT NULL,
                                                  	nm_fonte_de_recurso_tcm_2	TEXT NOT NULL,
                                                  	cod_fonte_de_recurso_gestor_2 TEXT NOT NULL,
                                                  	nm_fonte_de_recurso_gestor_2 TEXT NOT NULL,
                                                  	licitacao	TEXT NOT NULL,
                                                  	Dispensa_Inexigibilidade TEXT NOT NULL,
                                                  	contrato TEXT NOT NULL,
                                                  	declaracao TEXT NOT NULL,
                                                  	foreign_key	INTEGER NOT NULL,
                                                  	nm_arq_html_despesa	TEXT NOT NULL,
                                                  	hash_arq_html_despesa	TEXT NOT NULL,
                                                  	log_tratamento_arq_html_despesa	TEXT NOT NULL,
                                                  	link TEXT NOT NULL
                                                    );"
                                                    )
  
}

  DBI::dbDisconnect(connect_sgbd())

}


```


05 - Bloco de código (chunk) que contém a Função que cria e atualiza a tabela dCalendario no BD

```{r 05_criar_tab_dcalendario}

# Criar a função que gera a tabela com a relação de meses e ano para o Web Scraping;
criar_tb_dcalendario <- function(anos_alvos){

anos_alvos <- as.numeric(anos_alvos)

if(length(anos_alvos) > 1){ 
  
  return(print("Informe apenas um valor: O ano de início para realizar o Web Scraping."))

}

anos_validos <- c(2016, 2017, 2018)

if(!anos_alvos %in% anos_validos){
  
  return(print("Informe um dos seguintes anos: 2016, 2017 ou 2018"))
    
}

# Cria a conexão com o SGBD;
#conect_bd <- connect_sgbd()

tb_dcalendario <- purrr::map_dfr(anos_alvos, tb_anos_alvos) %>%
                  tibble::as.tibble()

DBI::dbWriteTable(connect_sgbd(), "tabela_dcalendario", tb_dcalendario, overwrite = TRUE)

print("A tabela `tabela_dcalendario` foi criada com sucesso no BD")

DBI::dbDisconnect(connect_sgbd())
  
  
}
 
######################################################################################
  
tb_anos_alvos <- function(anos_alvos) {
  

# Cria a tabela com a relação de meses e ano para o Web Scraping;
tb_calendario <- tibble::tibble(data = seq(ymd(paste0(anos_alvos,"-01-01")), (today() - day(today()) + 1 - months(2)), by = "month"),
                                ano = year(data),
                                mes = month(data),
                                log_create = log_data_hora()) %>%
                  dplyr::mutate(data = as.character(data),
                  # A coluna 'data', 'ano' e 'mês' foram transformados em 'character' 
                  # para ser registrada no SQLite como TEXT, já que ele não suporta o formato DATE;
                                ano = as.character(ano),
                                mes = as.character(mes))

return(tb_calendario)

}

######################################################################################

```


06 - Bloco de código (chunk) que contém as funções: 'criar_tb_dmunicipios' que gravará o resultado do scraping no BD na tabela 'tabela_tcm_dmunicipios'. E a 'scraping_tcm_municipios', responsável por capturar o código e nome dos municípios;


```{r 06_criar_tab_dmunicipios}

criar_tb_dmunicipios <- function() {

# Cria a conexão com o SGBD;
#conect_bd <- connect_sgbd()

print("Iniciando Web Scraping dos códigos e nomes dos Municípios utilziados pelo TCM-Ba")
  
# Executa o scraping que captura os códigos e nomes dos municípios
tb_tcm_dmunicipios <- scraping_tcm_municipios()


DBI::dbWriteTable(connect_sgbd(), "tabela_tcm_dmunicipios", tb_tcm_dmunicipios, overwrite = TRUE)

DBI::dbDisconnect(connect_sgbd())


print("A tabela `tabela_tcm_dmunicipios` foi criado com sucesso no BD")


}


######################################################################################


scraping_tcm_municipios <- function() {

# Scraping do código e nome dos municípios, e por meio do qual será feio o scraping
url_tcm <- url_tcm()


list_tcm_municipios <- httr::GET(url_tcm) %>% 
                       xml2::read_html() %>% 
                       rvest::html_nodes("#municipio > option") 

cod_municipio <- list_tcm_municipios %>% 
                 rvest::html_attr("value")

nm_municipio <- list_tcm_municipios %>% 
                rvest::html_text() %>% 
                stringr::str_replace(., "[*]", "") %>% 
                stringr::str_trim()

tabela_tcm_dmunicipios <- tibble::tibble(cod_municipio = cod_municipio,
                                         nm_municipio = nm_municipio,
                                         log_create = log_data_hora()) %>%
                          dplyr::filter(cod_municipio != "")

return(tabela_tcm_dmunicipios)


}

######################################################################################



```


07 - Bloco de código (chunk) que contém as funções que:'criar_tb_dmunicipios_entidades' que gravará o resultado do scraping no BD na tabela 'tabela_tcm_dmunicipios_entidades'.E a função 'scraping_tcm_entidades_ws', responsável por capturar o código e nome dos entes muninipais;


```{r 07_criar_tab_dmunicipios_entidades}

criar_tb_dmunicipios_entidades <- function() {


cod_nm_mun <- DBI::dbReadTable(connect_sgbd(), "tabela_tcm_dmunicipios")


#Gera a tabela com dados dos municípios e entidades, a partir do scraping do WS do TCM-Ba
scraping_mun_ent <- purrr::pmap_dfr(cod_nm_mun, scraping_tcm_entidades_ws)


DBI::dbWriteTable(connect_sgbd(), "tabela_tcm_dmunicipios_entidades", scraping_mun_ent, overwrite = TRUE)

DBI::dbDisconnect(connect_sgbd())


print("A tabela `tabela_tcm_dmunicipios_entidades` foi criado com sucesso no BD")


}

######################################################################################


scraping_tcm_entidades_ws <- function(cod_municipio, nm_municipio, log_create) {

# URL do Web Service do TCM-Ba;
url_tcm_entidades_ws <- url_tcm_entidades_ws()

# Scraping do código e nome dos entes municipais via Web Service;
resultado <- paste0(url_tcm_entidades_ws, cod_municipio) %>%
             httr::GET() %>%
             httr::content() %>%
             purrr::map_dfr(tibble::as_tibble)


# Tratamento dos dados obtidos via Web Service;
resultado_tratado <- resultado %>%
                    magrittr::set_names(c("cod_entidade", "nm_entidade")) %>%
                    dplyr::mutate(cod_municipio = cod_municipio, nm_municipio = nm_municipio,
                                  log_create = log_data_hora()) %>%
                    dplyr::mutate_all(stringr::str_to_upper) %>%
                    dplyr::mutate_all(stringr::str_trim) %>%
                    dplyr::select(cod_municipio, nm_municipio, cod_entidade, nm_entidade, log_create)


# Print para visualizar a progressão do scraping
print(paste("Scraping:", resultado_tratado$cod_municipio, "-", resultado_tratado$nm_municipio,
            "-", resultado_tratado$cod_entidade, "-", resultado_tratado$nm_entidade))


# Função return() para que o output seja o dataframe que está na variável 'resultado_tratado';
return(resultado_tratado)

}

######################################################################################



```



```{r outras_dimensoes}

criar_tb_ddespesas <- function() {


entidades <- DBI::dbReadTable(connect_sgbd(), "tabela_tcm_dmunicipios_entidades")


#Gera a tabela com dados dos municípios e entidades, a partir do scraping do WS do TCM-Ba
scraping_despesas <- purrr::pmap_dfr(entidades, scraping_tcm_despesas_ws)


DBI::dbWriteTable(connect_sgbd(), "tabela_tcm_ddespesas", scraping_despesas, overwrite = TRUE)

DBI::dbDisconnect(connect_sgbd())


print("A tabela `tabela_tcm_ddespesas` foi criado com sucesso no BD")


}

######################################################################################

scraping_tcm_despesas_ws <- function(cod_municipio, nm_municipio,
                                     cod_entidade, nm_entidade, ano) {
  
  # URL do Web Service do TCM-Ba;
url_tcm_despesas_ws <- paste0("https://www.tcm.ba.gov.br/Webservice/index.php/despesas/despesa?ent=", cod_entidade,
                              "&ano=", ano)

# Scraping do código e nome dos entes municipais via Web Service;
resultado <- httr::GET(url_tcm_despesas_ws) %>%
             httr::content() %>%
             purrr::map_dfr(tibble::as_tibble)


# Tratamento dos dados obtidos via Web Service;
resultado_tratado <- resultado %>%
                     magrittr::set_names(c("nm_despesa", "cod_despesa")) %>%
                     dplyr::select(cod_despesa, nm_despesa) %>%
                     dplyr::mutate(cod_municipio = cod_municipio,
                                   nm_municipio = nm_municipio,
                                   cod_entidade = cod_entidade,
                                   nm_entidade = nm_entidade,
                                   log_create = log_data_hora()) %>%
                     dplyr::mutate_all(stringr::str_to_upper) %>%
                     dplyr::mutate_all(stringr::str_trim) %>%
                     dplyr::mutate_all(funs(stringi::stri_trans_general(., "latin-ascii")))

# Print para visualizar a progressão do scraping
print(paste("Scraping:", resultado_tratado$cod_municipio, "-", resultado_tratado$nm_municipio,
            "-", resultado_tratado$cod_entidade, "-", resultado_tratado$nm_entidade))


# Função return() para que o output seja o dataframe que está na variável 'resultado_tratado';
return(resultado_tratado)


}

######################################################################################


criar_tb_dfontes_recursos <- function() {


entidades <- DBI::dbReadTable(connect_sgbd(), "tabela_tcm_dmunicipios_entidades")


#Gera a tabela com dados dos municípios e entidades, a partir do scraping do WS do TCM-Ba
scraping_fontes_recursos <- purrr::pmap_dfr(entidades, scraping_tcm_fontes_recursos_ws)


DBI::dbWriteTable(connect_sgbd(), "tabela_tcm_dfontes_recursos", scraping_fontes_recursos, overwrite = TRUE)

DBI::dbDisconnect(connect_sgbd())


print("A tabela `tabela_tcm_ddespesas` foi criado com sucesso no BD")


}

######################################################################################

scraping_tcm_fontes_recursos_ws <- function(cod_municipio, nm_municipio,
                                            cod_entidade, nm_entidade, ano) {
  
  # URL do Web Service do TCM-Ba;
url_tcm_recursos_ws <- paste0("https://www.tcm.ba.gov.br/Webservice/index.php/despesas/recurso?ent=", cod_entidade,
                              "&ano=", ano)

# Scraping do código e nome dos entes municipais via Web Service;
resultado <- httr::GET(url_tcm_recursos_ws) %>%
             httr::content() %>%
             purrr::map_dfr(tibble::as_tibble)


# Tratamento dos dados obtidos via Web Service;
resultado_tratado <- resultado %>%
                     magrittr::set_names(c("nm_recurso", "cod_recurso")) %>%
                     dplyr::select(cod_recurso, nm_recurso) %>%
                     dplyr::mutate(cod_municipio = cod_municipio,
                                   nm_municipio = nm_municipio,
                                   cod_entidade = cod_entidade,
                                   nm_entidade = nm_entidade,
                                   log_create = log_data_hora()) %>%
                     dplyr::mutate_all(stringr::str_to_upper) %>%
                     dplyr::mutate_all(stringr::str_trim) %>%
                     dplyr::mutate_all(funs(stringi::stri_trans_general(., "latin-ascii")))

# Print para visualizar a progressão do scraping
print(paste("Scraping:", resultado_tratado$cod_municipio, "-", resultado_tratado$nm_municipio,
            "-", resultado_tratado$cod_entidade, "-", resultado_tratado$nm_entidade))


# Função return() para que o output seja o dataframe que está na variável 'resultado_tratado';
return(resultado_tratado)

}


```






08 -  Bloco de código (chunk) que contém a Função que cria a tabela das entidades alvos

```{r 08_criar_tb_entidades_alvos_paginas}


criar_tb_entidades_alvos_paginas <- function(anos_alvos, cod_municipios_alvos){


criar_diretorios()

criar_tb_dcalendario(anos_alvos)


tb_dcalendario <- DBI::dbReadTable(connect_sgbd(), "tabela_dcalendario") %>%
                  dplyr::select("ano") %>%
                  tibble::as.tibble() %>%
                  dplyr::distinct()
                                  

tb_municipios_alvos_novos <- DBI::dbReadTable(connect_sgbd(), "tabela_tcm_dmunicipios_entidades") %>%
                             dplyr::filter(cod_municipio %in% cod_municipios_alvos) %>%
                             dplyr::mutate(pagina = "1") %>%
                             dplyr::select("cod_municipio", "nm_municipio",
                                           "cod_entidade", "nm_entidade", "pagina") %>%
                             tibble::as.tibble()
                         
# Essa parte do código foi um armenge, em virtude de não ter conseguido colocar o ano para expandir
# na rotina antior. Trocar por alguma função do dplyr
tb_municipios_alvos_novos <- merge.data.frame(tb_dcalendario, tb_municipios_alvos_novos) %>%
                             dplyr::mutate(filtro = paste0(ano, cod_municipio)) %>%
                             dplyr::arrange(desc(ano)) %>%
                             tibble::as.tibble()


tb_municipios_alvos_anteriores <- DBI::dbReadTable(connect_sgbd(), "tabela_entidades_alvos_paginas") %>%
                                  dplyr::mutate(filtro = paste0(ano, cod_municipio)) %>%
                                  tibble::as.tibble()


tb_municipios_alvos_atualizada <- tb_municipios_alvos_novos %>%
  # o sinal ! antes de cod_municipio é um
  # operador lógico para excluir os dados da coluna (Lei de De Morgan)
                                  dplyr::filter(!filtro %in% tb_municipios_alvos_anteriores$filtro) %>%
                                  dplyr::arrange(desc(ano)) %>%
                                  dplyr::mutate_all(funs(stringi::stri_trans_general(., "latin-ascii")))
#!!! Aqui, termina o armengue no código, pois não consegui usar a função dplyr::filter com dois critérios como um.
# No futuro, subistituir o techo aciima pelo debaixo que está comentado.


# tb_municipios_alvos_atualizada <- tb_municipios_alvos_novos %>%
#                                   dplyr::filter(!cod_municipio %in% cod_muni_ant | !ano %in% ano_ant)%>%
#                                   dplyr::arrange(desc(ano))

DBI::dbWriteTable(connect_sgbd(), "tabela_entidades_alvos_paginas",
                  tb_municipios_alvos_atualizada, append = TRUE)

DBI::dbDisconnect(connect_sgbd())


print("Tabela 'tabela_entidades_alvos_paginas' gerada com sucesso!")

}


```




09 - Bloco de código (chunk) que contém a Função do Web Scraping para obter as páginas e links das depesas


```{r 09_scraping_tab_paginas_links, message=FALSE, warning=FALSE}


executar_scraping_num_pags <- function() {

# Cria a conexão com o SGBD;
#conect_bd <- connect_sgbd()

entidades_alvos <- DBI::dbReadTable(connect_sgbd(), "tabela_entidades_alvos_paginas")

DBI::dbDisconnect(connect_sgbd())


print("Iniciando Web Scraping das páginas_links das entidades alvos!")


purrr::pwalk(entidades_alvos, scraping_num_pags)


print("Web Scraping das páginas_links das entidades alvos finalizado!")



}


######################################################################################

scraping_num_pags <- function(id, ano, cod_municipio, nm_municipio,
                              cod_entidade, nm_entidade, pagina, ...) {


  if (dir.exists(file.path("resposta_scraping_links", nm_municipio)) == FALSE) {
  dir.create(file.path("resposta_scraping_links", nm_municipio))
  }
      
  if (dir.exists(file.path("resposta_scraping_links", nm_municipio, nm_entidade)) == FALSE) {
  dir.create(file.path("resposta_scraping_links", nm_municipio, gsub("/", "", nm_entidade)))
  }
  
  
  repeat {
    

      site_tcm <- paste0("http://www.tcm.ba.gov.br/consulta-de-despesas/?pg=", pagina,
                        "&txtEntidade=&ano=", ano,
                        "&favorecido=&entidade=", cod_entidade,
                        "&orgao=&orcamentaria=&despesa=&recurso=&desp=P&dtPeriodo1=&dtPeriodo2=")
        

scraping_html_purrr <- purrr::safely(httr::GET)

scraping_tcm_paginas <- scraping_html_purrr(site_tcm, timeout(15))

# Grava a hora e data da requisição para ser incluída no arquivo HMTL e no BD 
log_request <- log_data_hora()


  # Verifica houve timeout. Se sim, esperar 10 segundos e tentar novamente.
if (length(scraping_tcm_paginas$result) == 0) {
  
   message("#### Erro: 'Timeout' da Primeira tentativa para a Página: ", pagina,
           " da entidade: ", nm_entidade, " ####")

    tb_request <- tibble::tibble(
                                 log_erro = "timeout - primeira tentativa",
                                 time = log_request,
                                 foreign_key = id,
                                 nm_entidade = nm_entidade,
                                 pagina = pagina,
                                 documento = "",
                                 arquivo_html = "",
                                 link = ""
                                 )
     
     

     DBI::dbWriteTable(connect_sgbd(), "tabela_log_pag", tb_request, append = TRUE)
     
     DBI::dbDisconnect(connect_sgbd())
     
     
     message("#### Iniciando Segunda tentativa para a Página: ",
             pagina, " da entidade: ", nm_entidade, " ...")
     
     # Pausa antes da segunad tentativa
     Sys.sleep(10)
     
     
     # Segunda tentativa. Se houver timeout novamente, pular para a próxima requisição.
     scraping_tcm_paginas <- scraping_html_purrr(site_tcm, timeout(15))
     
     log_request <- log_data_hora()
     
  
          if (length(scraping_tcm_paginas$result) == 0) {
      
               tb_request <- tibble::tibble(
                                            log_erro = "timeout - segunda tentativa",
                                            time = log_request,
                                            foreign_key = id,
                                            nm_entidade = nm_entidade,
                                            pagina = pagina,
                                            documento = "",
                                            arquivo_html = "",
                                            link = ""
                                            )
          
               DBI::dbWriteTable(connect_sgbd(), "tabela_log_pag", tb_request, append = TRUE)
          
               DBI::dbDisconnect(connect_sgbd())
                   
               
               # Parar a iteração e pular para a próxima requisição
               message("#### Erro: 'Timeout' Segunda tentativa para a Página: ", pagina,
                           "da entidade: ", nm_entidade,
                           "#### /n ###### Pulando para a próxima entidade ######")
               
               break
  

}

}

      
# Variável que será utilizada para verificar se há ou não tabela na página html   
verificador_tabela <- scraping_tcm_paginas$result %>%
                      xml2::read_html() %>%
                      rvest::html_node("#tabelaResultado") %>%
                      length()
      
    # Verifica se há tabela na página html
    if (verificador_tabela == 0 ) { 
        
        message("Não foi identificado tabela na Página: ", pagina, " da entidade: ", nm_entidade)
            
        break
            
}
  
    
# Verifica se há tabela, pelo seu tamanho, no HTML.
# Servirá como gatilho de parar o repeat quando chegar na última página
gatinho_to_break <- scraping_tcm_paginas$result %>%
                    xml2::read_html() %>%
                    rvest::html_node("#tabelaResultado") %>%
                    rvest::html_table() %>%
                    .$Documento %>%
                    length()

      # Se o gatilho de verificação for igual a 0, então ele pula para a próxima entidade municipal
if (gatinho_to_break == 0 ) { 
  
  message("Fim das requisições de ", nm_entidade, " na Página: ", pagina)
      
  break
            
}

      
# Gera o nome do arquivo
nm_arq_html_pag <- paste0(ano, "-", cod_entidade,
                          "-pag_", pagina, "_", ".html") %>%
                   stringr::str_replace_all("[/*]", "-")

tb_pag_links <- DBI::dbReadTable(connect_sgbd(), "tabela_paginas_links")

DBI::dbDisconnect(connect_sgbd())
      

      
    if (nm_arq_html_pag %in% tb_pag_links$nm_arq_html_pag & gatinho_to_break < 20) {
        
        
        message("Fim das requisições de ", nm_entidade, " na Página: ", pagina, " - P")
        
        break
        
}
      

    if (nm_arq_html_pag %in% tb_pag_links$nm_arq_html_pag & gatinho_to_break == 20) {
        
        print(paste0("Scraping - ", nm_entidade, " - ", ano," - Página: ", pagina, "- Continuação"))
        
        pegar_paginas <- scraping_tcm_paginas$result %>%
                         xml2::read_html() %>%
                         rvest::html_nodes("#tabelaResultado") %>%
                         xml2::write_html(file.path("resposta_scraping_links", nm_municipio,
                                                    gsub("/", "", nm_entidade), nm_arq_html_pag))       
          
       # Gera o Hash do Arquivo HTML que foi gravado
        hash_arq_html <- git2r::hashfile(file.path("resposta_scraping_links", nm_municipio,
                                                   gsub("/", "", nm_entidade), nm_arq_html_pag))
        
        pagina <- pagina + 1
        
        
        DBI::dbWithTransaction(connect_sgbd(), {
                               
        DBI::dbExecute(connect_sgbd(), 'UPDATE tabela_entidades_alvos_paginas 
                                        SET pagina = :pagina
                                        WHERE id = :id',
                       params = list(pagina = as.character(pagina),
                                     id = as.character(id)))
        
        DBI::dbExecute(connect_sgbd(), 'UPDATE tabela_paginas_links 
                                        SET arq_html_pag_tratado = "N",
                                            hash_arq_html_pag = :hash_arq_html
                                        WHERE nm_arq_html_pag = :nm_arq_html_pag',
                       params = list(hash_arq_html = as.character(hash_arq_html),
                                     nm_arq_html_pag = as.character(nm_arq_html_pag)))
        
}
)
        
        DBI::dbDisconnect(connect_sgbd())
        
        break
        
        
    } else {
       
      
       pegar_paginas <- scraping_tcm_paginas$result %>%
                        xml2::read_html() %>%
                        rvest::html_nodes("#tabelaResultado") %>%
                        xml2::write_html(file.path("resposta_scraping_links", nm_municipio,
                                                    gsub("/", "", nm_entidade), nm_arq_html_pag))       
          
       # Gera o Hash do Arquivo HTML que foi gravado
       hash_arq_html <- git2r::hashfile(file.path("resposta_scraping_links", nm_municipio,
                                                   gsub("/", "", nm_entidade), nm_arq_html_pag)) 
          
       tb_paginas_links <- tibble::tibble(ano = ano,
                                     cod_municipio = cod_municipio,
                                     nm_municipio = nm_municipio,
                                     cod_entidade = cod_entidade,
                                     nm_entidade = nm_entidade,
                                     pagina = pagina,
                                     status_request_html_pag = "S",
                                     log_request_html_pag = log_request,
                                     nm_arq_html_pag = nm_arq_html_pag,
                                     arq_html_pag_tratado = "N",
                                     hash_arq_html_pag = hash_arq_html,
                                     log_tratamento_arq_html_pag = "")
       
       
       
       DBI::dbWriteTable(connect_sgbd(), "tabela_paginas_links", tb_paginas_links, append = TRUE)
       
       DBI::dbDisconnect(connect_sgbd())

       
}       
       
   print(paste0("Scraping - ", nm_entidade, " - ", ano," - Página: ", pagina))
       
       
   if (gatinho_to_break < 20) {
   
   pagina <- pagina
       
}
   
   if (gatinho_to_break == 20) {
   
   pagina <- pagina + 1
       
}
       
  
DBI::dbExecute(connect_sgbd(), 'UPDATE tabela_entidades_alvos_paginas 
                                SET pagina = :pagina
                                WHERE id = :id',
               params = list(pagina = as.character(pagina),
                             id = as.character(id)))

DBI::dbDisconnect(connect_sgbd())
     

}     
        
 
}


######################################################################################



```


10 - Bloco de código (chunk) que contém a Função que cria a tabela de requisições


```{r 10_criar_tb_requisicoes_despesas}

######################################################################################

criar_tb_requisicoes_despesas <- function() {
  
  # Cria a conexão com o SGBD;
  #conect_bd <- connect_sgbd()
    

  tab_html_num_pags <- DBI::dbReadTable(connect_sgbd(), "tabela_paginas_links") %>%
                       dplyr::filter(arq_html_pag_tratado == "N")
  
  DBI::dbDisconnect(connect_sgbd())
  
  
  purrr::pwalk(tab_html_num_pags, parser_arq_html_pags)
  
  
  print("Tabela de requisições criada com sucesso")
  


}


######################################################################################


parser_arq_html_pags <- function(id, ano, cod_municipio, nm_municipio,
                                 cod_entidade, nm_entidade, pagina,
                                 status_request_html_pag, log_request_html_pag,
                                 nm_arq_html_pag, hash_arq_html_pag, ...) {


  # Registra o horário do parser para ser usado como registro do log na coluna 'log_tratamento_arq_html_pag'
  log_parser <- log_data_hora()
  
  # Realiza o parser no arquivo HTML
  parser_arq_html <- xml2::read_html(file.path("resposta_scraping_links", nm_municipio,
                                               gsub("/", "", nm_entidade), nm_arq_html_pag))
  
  # Extrai a tabela do arquivo HTML
  convert_html_tab <- parser_arq_html %>%
                      rvest::html_node("#tabelaResultado") %>% 
                      rvest::html_table()
  
  # Extrai os registros da coluna Documento da tabela
  doc_arq_html_pag <- convert_html_tab %>%
                      .$Documento %>%
                      stringr::str_replace_all("[/*]", "-") %>%
                      as.character()
  
  # Extrai os registros da coluna Empenho da tabela
  emp_arq_html_pag <- convert_html_tab %>%
                      .$Empenho %>%
                      as.character()
                      
  # Extrai os registros da coluna Valor da tabela
  valor_arq_html_pag <- convert_html_tab %>%
                        .$Valor %>%
                        readr::parse_number(locale = locale(grouping_mark = ".", decimal_mark = ","))
  
  # Extrai os links que estavam que estabam incorporados na tag 'a' da coluna Documento
  link_arq_html_pag <- parser_arq_html %>%
                       rvest::html_nodes("a") %>%
                       rvest::html_attrs() %>%
                       unlist()
  
  
  # Agrega todos os dados para formar a tabela de requisições
  tb_requisicoes <- tibble::tibble(
                                  ano = ano,
                                  cod_municipio = cod_municipio,
                                  nm_municipio = nm_municipio,
                                  cod_entidade = cod_entidade,
                                  nm_entidade = nm_entidade,
                                  pagina = pagina,
                                  status_request_html_pag = status_request_html_pag,
                                  log_request_html_pag = log_request_html_pag,
                                  nm_arq_html_pag = nm_arq_html_pag,
                                  arq_html_pag_tratado = "S",
                                  hash_arq_html_pag = hash_arq_html_pag,
                                  log_tratamento_arq_html_pag = log_parser,
                                  documento = doc_arq_html_pag,
                                  empenho = emp_arq_html_pag,
                                  valor_documento = valor_arq_html_pag,
                                  link_despesa = link_arq_html_pag,
                                  nm_arq_html_despesa = "",
                                  status_request_html_despesa = "N",
                                  log_request_html_despesa = "",
                                  arq_html_despesa_tratado = "N",
                                  hash_arq_html_despesa = "",
                                  log_tratamento_arq_html_despesa = ""
                                  )
  
  # Carrega a 'tabela_requisicoes' para servir de teste lógico na etapa seguinte
  tb_requisicoes_anterior <- DBI::dbReadTable(connect_sgbd(), "tabela_requisicoes") %>%
                             dplyr::filter(cod_entidade == cod_entidade & pagina == pagina)
  
  DBI::dbDisconnect(connect_sgbd())
  
   
  tb_requisicoes_atualizada <- tb_requisicoes %>%
  # Exclui as URL iguais que já existiam na tabela, proveniente de HTML com informações parciais
                               dplyr::filter(!link_arq_html_pag %in% tb_requisicoes_anterior$link_despesa)
  
  
# Garante que as duas operações a seguir serão executadas no SGBD. Caso contrário, não altera o BD
DBI::dbWithTransaction(connect_sgbd(), {
  
  # Grava a tabela 'tb_requisicoes' no Bando de Dados na tabela 'tabela_requisicoes'
  DBI::dbWriteTable(connect_sgbd(), "tabela_requisicoes", tb_requisicoes_atualizada, append = TRUE)
  
  # Grava "S" na tabela '' para controlar os arquivos HTML já tratados 
  DBI::dbExecute(connect_sgbd(), 'UPDATE tabela_paginas_links 
                                  SET arq_html_pag_tratado = "S",
                                      log_tratamento_arq_html_pag = :log_parser
                                  WHERE cod_entidade = :cod_entidade AND
                                        pagina = :pagina;',
                 params = list(log_parser = as.character(log_parser),
                               cod_entidade = as.character(cod_entidade),
                               pagina = as.character(pagina)))
  
}
)

DBI::dbDisconnect(connect_sgbd())


  print(paste("Parser:", nm_arq_html_pag, "-", nm_entidade))
  
  

}


```



11 - Bloco de código (chunk) que contém a Função que faz o Web Scraping para obter os arquivos HMTL que contêm os Extatos das Despesas 

```{r 11_scraping_html_despesas}

executar_scraping_html_despesas <- function() {
  
  # Cria a conexão com o SGBD;
  #conect_bd <- connect_sgbd()
  

  tb_requisicoes <- DBI::dbReadTable(connect_sgbd(), "tabela_requisicoes") %>%
                    dplyr::filter(status_request_html_despesa == "N")
  
  DBI::dbDisconnect(connect_sgbd())
  
  
  message("Iniciando Web Scraping dos arquivos HTML das Despesas")
  
  
  purrr::pwalk(tb_requisicoes, scraping_html_despesas)
  
  
  message("O Web Scraping dos arquivos HTML das Despesas foi concluído")
  
  
  # Rotina para executar uma segunda tentativa do Scraping
  # para requisitar as URL com timeout ou 404, ou os HTML com inconsistência 
  tb_requisicoes_2 <- DBI::dbReadTable(connect_sgbd(), "tabela_requisicoes") %>%
                      dplyr::filter(status_request_html_despesa == "N")
  
  DBI::dbDisconnect(connect_sgbd())
  

  if (nrow(tb_requisicoes_2) > 0){
    
    message("Segunda tentativa para requisitar as URL com 'timeout' ou 404, ou os HTML com inconsistência")
    
    purrr::pwalk(tb_requisicoes_2, scraping_html_despesas)
  
    message(" A Segunda tentativa do Web Scraping dos arquivos HTML das Despesas foi concluída")
    
    DBI::dbDisconnect(connect_sgbd())
    
    
}


}


######################################################################################


scraping_html_despesas <- function(id, ano, cod_municipio, nm_municipio,
                                   cod_entidade, nm_entidade, pagina,
                                   status_request_html_pag, log_request_html_pag,
                                   nm_arq_html_pag, documento, valor_documento,
                                   link_despesa, ...) {
  
  if (dir.exists(file.path("resposta_scraping_html", nm_municipio)) == FALSE) {
      dir.create(file.path("resposta_scraping_html", nm_municipio))
  }
      
  if (dir.exists(file.path("resposta_scraping_html", nm_municipio, nm_entidade)) == FALSE) {
      dir.create(file.path("resposta_scraping_html", nm_municipio, gsub("/", "", nm_entidade)))
  }

  

scraping_html_purrr <- purrr::safely(httr::GET)

scraping_html <- scraping_html_purrr(link_despesa, timeout(35))

log_request <- log_data_hora()


  # Verifica houve timeout. Se sim, esperar 20 segundos e tentar novamente.
if (length(scraping_html$result) == 0) {
  
    message("#### Erro: 'Timeout' da Primeira tentativa para: ",
             nm_arq_html_pag, " - doc: ", documento, " ####")
  
     tb_request <- tibble::tibble(
                                  log_erro = "timeout - primeira tentativa",
                                  time = log_request,
                                  foreign_key = id,
                                  nm_entidade = nm_entidade,
                                  pagina = pagina,
                                  documento = documento,
                                  arquivo_html = "",
                                  link = link_despesa
                                  )
     

   
     DBI::dbWriteTable(connect_sgbd(), "tabela_log", tb_request, append = TRUE)
     
     DBI::dbDisconnect(connect_sgbd())
     
     
      message("#### Iniciando Segunda tentativa para: ",
              nm_arq_html_pag, " - doc: ", documento, " ...")
     
     # Pausa antes da segunad tentativa
     Sys.sleep(10)
     
     
     # Segunda tentativa. Se houver timeout novamente, pular para a próxima requisição.
     scraping_html <- scraping_html_purrr(link_despesa, timeout(35))
     
     log_request <- log_data_hora()
     
  
          if (length(scraping_html$result) == 0) {
      
              tb_request <- tibble::tibble(
                                          log_erro = "timeout - segunda tentativa",
                                          time = log_request,
                                          foreign_key = id,
                                          nm_entidade = nm_entidade,
                                          pagina = pagina,
                                          documento = documento,
                                          arquivo_html = "",
                                          link = link_despesa
                                          )
              
               DBI::dbWriteTable(connect_sgbd(), "tabela_log", tb_request, append = TRUE)
          
               DBI::dbDisconnect(connect_sgbd())
               
               
               # Parar a iteração e pular para a próxima requisição
               return(message("#### Erro: 'Timeout' da Segunda tentativa para: ", nm_arq_html_pag,
                                  " - doc: ", documento, " - Pulando para o próximo link de despesa ####"))
  

}

}


# Verifica se há erro de querisição 404. Se sim, grava o erro numa tabela de log no BD.
if (scraping_html$result$status_code == 404) {

  
     tb_request <- tibble::tibble(
                                  log_erro = "erro - 404",
                                  time = log_request,
                                  foreign_key = id,
                                  nm_entidade = nm_entidade,
                                  pagina = pagina,
                                  documento = documento,
                                  arquivo_html = "",
                                  link = link_despesa
                                  )
  

     DBI::dbWriteTable(connect_sgbd(), "tabela_log", tb_request, append = TRUE)
     
     DBI::dbDisconnect(connect_sgbd())
     
  
     # Parar a iteração e pular para a próxima requisição.
     return(message("#### Erro 404 de Requisição para: ",
                    nm_arq_html_pag, " - doc: ", documento,
                    " - Pulando para o próximo link de desepesa ####"))
 
        
}


# Realiza um teste no HTML para saber se os dados estão completos, ou se houve erro durante a resposta do TCM
# scraping_html$result é proveniente da função 'scraping_html_purrr'
  teste_html_despesas <- scraping_html$result %>%
                         xml2::read_html() %>%
                         rvest::html_nodes("label+ span") %>%
                         rvest::html_text() %>%
                         stringr::str_trim()
  
  # Primeiro critério que será usado no teste de integridade do arquivo HTML
  teste <- "-"

    # Retirei o critério 'teste_html_despesas[13] == teste_1', pois o arquivo está com 
  # a informação '-' na base de dados do TCM
if (teste_html_despesas[8] == teste | teste_html_despesas[8] == "" | is.na(teste_html_despesas[8])) {


     nm_arquivo_html_log <- paste0("log_", ano, "-", cod_entidade,
                                   "-pag_", pagina, "-doc_", documento,
                                   "-val_", valor_documento, "_.html")
  
     pegar_arquivo_html_log <- scraping_html$result %>%
                               xml2::read_html() %>%
                               rvest::html_node("div.col-xs-12.content.padding_content") %>%
                               xml2::write_html(file.path("log_html", nm_arquivo_html_log))
     
 
     tb_request <- tibble::tibble(
                                  log_erro = "HTML de despesa incompleto",
                                  time = log_request,
                                  foreign_key = id,
                                  nm_entidade = nm_entidade,
                                  pagina = pagina,
                                  documento = documento,
                                  arquivo_html = nm_arquivo_html_log,
                                  link = link_despesa
                                  )
  
     
     DBI::dbWriteTable(connect_sgbd(), "tabela_log", tb_request, append = TRUE)
     
     DBI::dbDisconnect(connect_sgbd())
     
     
     return(message("### O HTML ", nm_arq_html_pag, " - doc: ",
                        documento, " não está com informações completas. Tentar mais tarde. ###"))
   
     
# Se tudo estiver OK com a requisição e com o arquivo HTML, então executa esse bloco de código.
  } else {
      
  # Salva o arquivo HTML no HD para ser tratado por outra função    
    nome_arquivo_html <- paste0(ano, "-", cod_entidade,
                                "-pag_", pagina, "-doc_", documento,
                                "-val_", valor_documento, "_.html") %>%
                         stringr::str_replace_all("[/*]", "-")

        if (file.exists(file.path("resposta_scraping_html", nm_municipio,
                                  gsub("/", "", nm_entidade), nome_arquivo_html)) == TRUE) {
    
              sufixo <- format(Sys.time(), "%H_%M_%S")
              
              nome_arquivo_html <- paste0(gsub("_.html", "", nome_arquivo_html),
                                          "-d_", sufixo, "_.html")
            
}
              
    
    # scraping_html$result é proveniente da função 'scraping_html_purrr'
    pegar_html_despesas <- scraping_html$result %>%
                           xml2::read_html() %>%
                           rvest::html_node("div.col-xs-12.content.padding_content") %>%
                           xml2::write_html(file.path("resposta_scraping_html", nm_municipio,
                                                      gsub("/", "", nm_entidade), nome_arquivo_html))

    
    # Gera o Hash do Arquivo HTML que foi gravado
    hash_arq_html_despesa <- git2r::hashfile(file.path("resposta_scraping_html", nm_municipio,
                                                       gsub("/", "", nm_entidade), nome_arquivo_html))
    

    
    # Grava "S" na tabela 'tabela_paginas_links' para controlar os arquivos HTML já tratados 
    DBI::dbExecute(connect_sgbd(), 'UPDATE tabela_requisicoes 
                                    SET status_request_html_despesa = "S",
                                        nm_arq_html_despesa = :nome_arquivo_html,
                                        log_request_html_despesa = :log_request,
                                        hash_arq_html_despesa = :hash_arq_html_despesa
                                    WHERE id = :id;',
                   params = list(nome_arquivo_html = as.character(nome_arquivo_html),
                                 log_request = as.character(log_request),
                                 hash_arq_html_despesa = as.character(hash_arq_html_despesa),
                                 id = as.character(id)))
    
    DBI::dbDisconnect(connect_sgbd())
    
    print(paste("Scraping -", "Ano:", ano, "- Pág:", pagina, "- Doc:", documento, 
                  "- Valor:", valor_documento, "-", nm_entidade))

}

    
}


######################################################################################


```



12 - Bloco de código (chunk) que contém a Função que faz o parser e o Data Munging dos arquivos HMTL que contêm os Extatos das Despesas. E função que transforma os dados no padrão Tidy, armazenando-os em uma tabela no BD e exportando em arquivos CSV.



```{r 12_data_wrangling_html_despesas}

executar_data_wrangling_html_despesas <- function() {
  

tb_requisicoes <- DBI::dbReadTable(connect_sgbd(), "tabela_requisicoes") %>%
                  dplyr::filter(arq_html_despesa_tratado == "N" & nm_arq_html_despesa != "")
  
DBI::dbDisconnect(connect_sgbd())

message("Todos os Arquivos HTML das despesas foram tratados com sucesso!")

  
  if (nrow(tb_requisicoes) == 0) {
    
        message("Todos os Arquivos HTML das despesas já foram tratados")
    
    } else {

        
        purrr::pwalk(tb_requisicoes, data_wrangling_html_despesas)
      
        # future::plan(multisession)
        # 
        # furrr::future_pmap(tb_requisicoes, data_wrangling_html_despesas, .progress = TRUE)
        
        # abjutils::pvec()

        DBI::dbDisconnect(connect_sgbd())
        
        message("Todos os Arquivos HTML das despesas já foram tratados")

}

  
}


######################################################################################


data_wrangling_html_despesas <- function(id, ano, cod_municipio, nm_municipio,
                                cod_entidade, nm_entidade, pagina,
                                status_request_html_pag, log_request_html_pag,
                                nm_arq_html_pag, documento, valor_documento,
                                link_despesa, nm_arq_html_despesa,
                                hash_arq_html_despesa,
                                log_tratamento_arq_html_despesa, ...){

 # Realiza o parser no arquivo HTML
parser_arq_html <- xml2::read_html(file.path("resposta_scraping_html", nm_municipio,
                                             gsub("/", "", nm_entidade), nm_arq_html_despesa),
                                   encoding = "UTF-8")


pegar_dados_html <- parser_arq_html %>%
                    rvest::html_nodes("label+ span") %>%
                    rvest::html_text() %>%
                    stringr::str_trim()


log_parser_arq_html_despesa <- log_data_hora()


tb_despesas_municipais <- tibble::tibble(
                                        fase = pegar_dados_html[1],
                                        data_do_pagamento = pegar_dados_html[2],
                                        valor_do_pagamento = pegar_dados_html[3],
                                        documento = pegar_dados_html[4],
                                        empenho = pegar_dados_html[5],
                                        data_empenho = pegar_dados_html[6],
                                        tipo_de_empenho = pegar_dados_html[7],
                                        favorecido = pegar_dados_html[8],
                                        valor_do_empenho = pegar_dados_html[9],
                                        valor_das_retencoes = pegar_dados_html[10],
                                        restos_a_pagar = pegar_dados_html[11],
                                        conta_bancaria = pegar_dados_html[12],
                                        fonte_de_recurso_tcm = pegar_dados_html[13],
                                        fonte_de_recurso_gestor = pegar_dados_html[14],
                                        tipo_de_documento = pegar_dados_html[15],
                                        # Enriqueci a tabela com o dado do código do município
                                        cod_municipio = cod_municipio,
                                        municipio = pegar_dados_html[16],
                                        # Enriqueci a tabela com o dado do código da entidade municipal
                                        cod_entidade = cod_entidade,
                                        nm_entidade = pegar_dados_html[17],
                                        poder = pegar_dados_html[18],
                                        orgao = pegar_dados_html[19],
                                        unidade_orcamentaria = pegar_dados_html[20],
                                        funcao = pegar_dados_html[21],
                                        subfuncao = pegar_dados_html[22],
                                        programa = pegar_dados_html[23],
                                        tipo_acao = pegar_dados_html[24],
                                        acao = pegar_dados_html[25],
                                        natureza_da_despesa_tcm = pegar_dados_html[26],
                                        natureza_da_despesa_gestor = pegar_dados_html[27],
                                        fonte_de_recurso_tcm_2 = pegar_dados_html[28],
                                        fonte_de_recurso_gestor_2 = pegar_dados_html[29],
                                        licitacao = pegar_dados_html[30],
                                        dispensa_inexigibilidade = pegar_dados_html[31],
                                        contrato = pegar_dados_html[32],
                                        declaracao = pegar_dados_html[33],
                                        foreign_key = id,
                                        nm_arq_html_despesa = nm_arq_html_despesa,
                                        hash_arq_html_despesa = hash_arq_html_despesa,
                                        log_tratamento_arq_html_despesa = log_parser_arq_html_despesa,
                                        link = link_despesa
                                        )
                                      

DBI::dbWithTransaction(connect_sgbd(), {

DBI::dbWriteTable(connect_sgbd(), "tabela_despesas_municipais", tb_despesas_municipais, append = TRUE)

# Grava "S" na tabela 'tabela_requisicoes' para controlar os arquivos HTML já tratados
DBI::dbExecute(connect_sgbd(), 'UPDATE tabela_requisicoes
                                SET arq_html_despesa_tratado = "S",
                                    log_tratamento_arq_html_despesa = :log_parser_arq_html_despesa
                                WHERE id = :id;',
               params = list(log_parser_arq_html_despesa = as.character(log_parser_arq_html_despesa),
                             id = as.character(id)))

}
)

DBI::dbDisconnect(connect_sgbd())

return(print(paste("Tratado:", nm_arq_html_despesa, "-", nm_entidade)))


}


######################################################################################


  


executar_tidy_data <- function() {


  tb_despesas_municipios <- DBI::dbReadTable(connect_sgbd(), "tabela_despesas_municipais") %>%
                            tibble::as.tibble() %>%
                            tidyr::separate(col = favorecido,
                                            into = c("cod_favorecido",
                                                     "nm_favorecido"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>%    
                            tidyr::separate(col = fonte_de_recurso_tcm,
                                            into = c("cod_fonte_de_recurso_tcm",
                                                     "nm_fonte_de_recurso_tcm"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>%
                            tidyr::separate(col = fonte_de_recurso_gestor,
                                            into = c("cod_fonte_de_recurso_gestor",
                                                     "nm_fonte_de_recurso_gestor"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>%
                            tidyr::separate(col = orgao,
                                            into = c("cod_orgao",
                                                     "nm_orgao"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>%
                            tidyr::separate(col = unidade_orcamentaria,
                                            into = c("cod_unidade_orcamentaria",
                                                     "nm_unidade_orcamentaria"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>%
                            tidyr::separate(col = funcao,
                                            into = c("cod_funcao",
                                                     "nm_funcao"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>% 
                            tidyr::separate(col = subfuncao,
                                            into = c("cod_subfuncao",
                                                     "nm_subfuncao"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>% 
                            tidyr::separate(col = programa,
                                            into = c("cod_programa",
                                                     "nm_programa"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>% 
                            tidyr::separate(col = tipo_acao,
                                            into = c("cod_tipo_acao",
                                                     "nm_tipo_acao"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>%
                            tidyr::separate(col = acao,
                                            into = c("cod_acao",
                                                     "nm_acao"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>% 
                            tidyr::separate(col = natureza_da_despesa_tcm,
                                            into = c("cod_natureza_da_despesa_tcm",
                                                     "nm_natureza_da_despesa_tcm"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>%
                            tidyr::separate(col = natureza_da_despesa_gestor,
                                            into = c("cod_natureza_da_despesa_gestor",
                                                     "nm_natureza_da_despesa_gestor"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>%
                            tidyr::separate(col = fonte_de_recurso_tcm_2,
                                            into = c("cod_fonte_de_recurso_tcm_2",
                                                     "nm_fonte_de_recurso_tcm_2"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>%
                            tidyr::separate(col = fonte_de_recurso_gestor_2,
                                            into = c("cod_fonte_de_recurso_gestor_2",
                                                     "nm_fonte_de_recurso_gestor_2"),
                                            sep = " - ",
                                            remove = TRUE, extra = "merge") %>%
                            dplyr::mutate(valor_das_retencoes = stringr::str_replace(valor_das_retencoes, "-", "0")) %>%
                            dplyr::mutate(tipo_de_documento = stringr::str_replace(tipo_de_documento, "[º]", ".")) %>%
                            dplyr::mutate_at(vars(valor_do_pagamento, valor_das_retencoes,
                                                  valor_do_empenho), ~valor_monetario(.)) %>%
                            dplyr::mutate_at(vars(data_do_pagamento, data_empenho), ~lubridate::dmy(.)) %>%
                            dplyr::mutate_at(vars(fase:declaracao), ~stringr::str_to_upper(.)) %>%
                            dplyr::mutate_all(stringr::str_trim) %>%
                            dplyr::mutate_all(funs(stringi::stri_trans_general(., "latin-ascii")))

#Desconectar do DBI::dbReadTable
DBI::dbDisconnect(connect_sgbd())
  
DBI::dbWriteTable(connect_sgbd(), "tabela_despesas_municipais_tidy_data", tb_despesas_municipios, overwrite = TRUE)

DBI::dbDisconnect(connect_sgbd())

message("Os dados foram colocados no padrão Tidy Data e salvos no Bando de Dados em 'tabela_despesas_municipais_tidy_data'")


readr::write_delim(tb_despesas_municipios, file.path("dados_exportados",
                                                     "tabela_despesas_municipais_tidy_data.csv"), delim = ";")

message("Os dados foram colocados no padrão Tidy Data Internacional e salvos em CSV no diretório 'dados_exportados'")


tb_despesas_municipios_BR <- tb_despesas_municipios %>%
                             dplyr::mutate_at(vars(valor_das_retencoes, valor_do_pagamento,
                                                   valor_do_empenho), ~stringr::str_replace(., "[.]", ","))


readr::write_delim(tb_despesas_municipios_BR, file.path("dados_exportados",
                                                        "tabela_despesas_municipais_tidy_data_BR.csv"), delim = ";")


message("Os dados foram colocados no padrão Tidy Data Brasil (R$) e salvos em CSV no diretório 'dados_exportados'")


# Rotinas para compactar os arquivos CSV em '.gz' e '.zip'.

readr::write_delim(tb_despesas_municipios_BR, file.path("dados_exportados",
                                                        "tabela_despesas_municipais_tidy_data_BR.csv.gz"), delim = ";")

zip::zip(file.path("dados_exportados", "tabela_despesas_municipais_tidy_data_BR.zip"),
         file.path("dados_exportados", "tabela_despesas_municipais_tidy_data_BR.csv"), compression_level = 9)

message("O arquivo CSV com os dados no formato Tidy Data Brasil (R$) foram compactados em '.gz' e '.zip' no diretório 'dados_exportados'")


# jsonlite::write_json(tb_despesas_municipios, file.path("dados_exportados",
#                                                       "tabela_despesas_municipais_tidy_data.json"))
# 
# 
# message("Os dados foram colocados no padrão Tidy Data Internacional e salvos em JSON no diretório 'dados_exportados'")
# 
# 
# jsonlite::write_json(tb_despesas_municipios_BR, file.path("dados_exportados",
#                                                       "tabela_despesas_municipais_tidy_data_BR.json"))
# 
# 
# message("Os dados foram colocados no padrão Tidy Data Brasil (R$) e salvos em JSON no diretório 'dados_exportados'")


}

  

######################################################################################


```


13 - 

```{r 13_executar_csv_googledrive}


executar_csv_googledrive <- function() {
  
  
  nome_arquivo_csv <- "tabela_despesas_municipais_tidy_data_BR.csv"
  
  dir_arquivo_csv <- file.path("dados_exportados", "tabela_despesas_municipais_tidy_data_BR.csv")
  
  if (file.exists(dir_arquivo_csv) == FALSE) {
    
    print("O Arquivo CSV ainda não foi criado!")
    
    break

}
  

  # drive_upload_purrr <- purrr::safely(googledrive::drive_upload)
  
  #!!! Será preciso armazenar as ID dos arquivos numa tabela após o upload para poder viabilziar o update posteriormente
  #    Ou usar alguma outra estratégia ou função do pacote googledrive

  # resultado_googledrive <- drive_upload_purrr(dir_arquivo_csv,
  #                                   googledrive::as_id(""),
  #                                   name = nome_arquivo_csv)
  



#   # Grava a hora e data da requisição para ser incluída no arquivo HMTL e no BD 
#   log_request <- log_data_hora()
#   
#   
#   if (length(resultado_upload$error) > 0) {
#   
#     message("#### Erro: No UPLOAD do arquivo CSV para o Google Drive ####")
# 
#     tb_request <- tibble::tibble(
#                                  log_erro = "google_drive: erro no upload do CSV",
#                                  time = log_request,
#                                  foreign_key = "",
#                                  nm_entidade = "",
#                                  pagina = "",
#                                  documento = "",
#                                  arquivo_html = "",
#                                  link = ""
#                                  )
#      
#      
#      DBI::dbWriteTable(connect_sgbd(), "tabela_log_pag", tb_request, append = TRUE)
#      
#      DBI::dbDisconnect(connect_sgbd())
# }
#   

googledrive::drive_update(googledrive::as_id(""), dir_arquivo_csv)

print("Arquivo CSV exportado com sucesso para o Google Drive!")
  
  
}



```



14 - Bloco de código (chunk) que contém a Função que faz o Backup de todas as pastas utilizas no Web Scraping. Em seguida, o arquivo é enviado para um link de acesso público do Google Drive;



```{r 14_executar_backup_googledrive}


executar_backup_googledrive <- function() {
  
  # Define o padrão do nome a ser utilizado para identificar o BK dos arquivos
  sufixo <- log_data_hora() %>%
            stringr::str_replace_all("[:]", "-") %>%
            stringr::str_replace_all("[ ]", "_")
              
  nome_arquivo_bk <- file.path("backup", paste0("bk_do_tcm_despesas_municipais_", sufixo, ".zip"))
  
  zip::zip(nome_arquivo_bk, getwd())
  
  # Grava a hora e data da requisição para ser incluída no arquivo HMTL e no BD 
  log_request <- log_data_hora()
  
  drive_upload_purrr <- purrr::safely(googledrive::drive_upload)
  
  resultado_upload <- drive_upload_purrr(nome_arquivo_bk,
                                         googledrive::as_id("XXXXXXXXXXXXXXXXXXXXX"))
  

  if (length(resultado_upload$error) > 0) {
  
    message("#### Erro: No UPLOAD do arquivo ZIP para o Google Drive ####")

    tb_request <- tibble::tibble(
                                 log_erro = "google_drive: erro no upload do Backup 'ZIP'",
                                 time = log_request,
                                 foreign_key = "",
                                 nm_entidade = "",
                                 pagina = "",
                                 documento = "",
                                 arquivo_html = "",
                                 link = ""
                                 )
     
     
     DBI::dbWriteTable(connect_sgbd(), "tabela_log_pag", tb_request, append = TRUE)
     
     DBI::dbDisconnect(connect_sgbd())
}
  

print("Arquivo do Backup 'ZIP' exportado com sucesso para o Google Drive!")
  
  
  

}



```

15 - Bloco de código (chunk) que contém a Função que será utilizada com API, via pacote Plumber, para exportar JSON e CSV;


```{r 15_API}

# Script do API que será colocado no arquivo 'api_csv.R' na pasta de trabalho do R
#-------------------------------------------------------



#* @get /dados-tcm-despesas-csv.csv
api_csv <- function(req, res) {
  
  dados <- file.path("home", "rstudio", "os_saj_web_scraping", "tcm_despesas_municipais",
                     "dados_exportados", "tabela_despesas_municipais_tidy_data_BR.csv")
  
  plumber::include_file(dados, res, "text/csv")
    
}

#----------------------------------------------------------

#Comando que executará o API que está no arquivo 'api_csv.R'
#----------------------------------------------------------

executar_api_csv <- plumber::plumb("api_csv.R")

executar_api_csv$run(host = "0.0.0.0", port = 8000)


######################################################################################
######################################################################################

#Servidor e Arquivo Estático
#----------------------------------------------------------

#* @assets ./home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/dados_exportados

list()


#Comando de teste
#----------------------------------------------------------

# http://IP:8000/public/tabela_despesas_municipais_tidy_data_BR.csv


######################################################################################
######################################################################################

#Comando que executará o API que está no arquivo 'api_json.R'
#----------------------------------------------------------

#* @get /dados_tcm_despesas_json

api_json <- function(req, res) {
  
    dados <- file.path("dados_exportados", "tabela_despesas_municipais_tidy_data_BR.json")
    
}

#----------------------------------------------------------

#Comando que executará o api que está no arquivo 'api_json.R'
#----------------------------------------------------------

executar_api_json <- plumber::plumb("api_csv.R")

executar_api_json$run(port = 8889)




```




- Área de Execução para teste do código


```{r executar, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE, output=FALSE}


# Variável que define a partir de que ano os dados serão raspados.
anos_alvos <- c("2018")

# Variável que define os municípios (e respectios entes municipais) serão raspados.
# Web Scraping das entidades municipais somente do Município de Salvador (Prefeitura, Câmara e Adm. Indireta)
#cod_municipios <- c(2928703)

# Web Scraping das entidades municipais dos Municípios de:
# Santo Antõnio de Jesus, Santo Amaro, Nazaré, Porto Seguro, Ilhéus, Barreiras, São Francisco do Conde, Salvador e Jequié
# cod_municipios <- c(2928703, 2928604, 2922508, 2925303, 2913606, 2903201, 2929206, 2927408, 2918001)

# Santo Antõnio de Jesus, Santo Amaro, Nazaré, Dom Macedo Costa, Porto Seguro, Ilhéus, Barreiras, São Francisco do Conde, Jequié, Catu, Valença e Lauro de Freitas, Irecê, Ipiaú, Feira de Santana, Salvador
 cod_municipios <- c(2928703, 2928604, 2910206, 2922508, 2925303, 2913606, 2903201, 2929206, 2918001, 2907509, 2919207, 2918407, 2933307, 2905701, 2914802, 2931350, 2900702, 2930709, 2924009, 2910727, 2932903, 2906501, 2911709)

# Função que cria as pastas dos arquivos
criar_diretorios()

# Função que cria 4 tabelas que serão armazenadas no SQLite.
criar_tabelas_bd()

# Função que cria a tabela dCalendario
criar_tb_dcalendario(anos_alvos)

# Função que faz o Web Scraping do código e nome dos Municípios
criar_tb_dmunicipios()

# Função que faz o Web Scraping (via Web Service) do código e nome das Entidades e, ao fim, cria a tabela.
criar_tb_dmunicipios_entidades()

# Função que cria a tabela das entidades alvos, controlando a paginação por cod_municipio e Ano.
criar_tb_entidades_alvos_paginas(anos_alvos, cod_municipios)

# Função executa o Web Scraping das páginas que têm os links que darão acesso
# as páginas HTML que contêm os dados sobre as despesas
executar_scraping_num_pags()

# Função que Cria a tabela central para Controle das Requisições dos HTML que contêm
# as informções sobre as despesas municipais
criar_tb_requisicoes_despesas()

# Função cria a tabela de requisições e faz o Web Scraping das páginas HTML que contêm
# os dados das despesas. #OBS: O tempo de resposta do TCM está entre 10 a 30 segundos
executar_scraping_html_despesas()

# Função que faz o parser dos HTMLs das depesas e o Data Wrangling dos HTMLs
executar_data_wrangling_html_despesas()

# Função que faz o pré-processamento dos dados obtidos do HTML, aplicando o conceito Tidy Data 
# Por fim, cria uma tabela no padrão Tidy Data no BD e exporta dois arquivos CSV para a pasta
# dados_exportados. Um arquivo no padrão Tidy Internacional e outro no padrão Brasil (R$).
executar_tidy_data()

                    # O conceito Tidy Data de Hadley Wickham tem por objetivo arrumar os dados
                    # para que eles sejam utilizados em softwares de estatísticas ou
                    # de Business Intelligence sem a necessidade de realizar
                    # mais transformações nos dados.
                    
                    # O Conceito está resumido nestas três regras:
                    # - Cada variável deve ter sua própria coluna.
                    # - Cada observação deve ter sua própria linha.
                    # - Cada valor deve ter sua própria célula.
                    
                    #(http://r4ds.had.co.nz/tidy-data.html)


# Função que exporta os CSVs para o Google Drive
executar_csv_googledrive()


# Função que faz o Backup dos Arquivos e exporta para o Google Drive
executar_backup_googledrive()


```






# Docker

------------------ Pré-Configuração do Container Docker do Rstudio na Digital Ocean ------------------
_________________________________________________________________________________
```

#cloud-config
runcmd:

docker run -d -t -p 8787:8787 --name=web_scraping_ossaj -e ROOT=TRUE -e PASSWORD=senhadoUsuario -v /home/rstudio/os_saj_web_scraping:/home/rstudio/os_saj_web_scraping rocker/tidyverse
  
sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping

sudo chmod -R 770 /home/rstudio/os_saj_web_scraping

adduser ws_despesa

adduser ws_pessoal


sudo apt-get update

sudo apt-get install -y cron

sudo cron start

```
_________________________________________________________________________________


------------------ Configuração do Container Docker do Rstudio no Shell do Linux ------------------

- Levantar um container com pasta Volume (3 etapas)
```

docker run -d -t -p 8787:8787 --name=web_scraping_ossaj -e ROOT=TRUE -e PASSWORD=senhadoUsuario -v /home/rstudio/os_saj_web_scraping:/home/rstudio/os_saj_web_scraping rocker/tidyverse

docker exec -d web_scraping_ossaj sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping

docker exec -d web_scraping_ossaj sudo chmod -R 770 /home/rstudio/os_saj_web_scraping


docker exec -it web_scraping_ossaj bash

adduser ws_despesa

adduser ws_pessoal


```

------------------------------------ Docker File Personalizado ------------------------------------


Exemplo Super básica fácil: Este é rocker/tidyverse com as versões CRAN httr, plumber, git2r, janitor, zip e googledrive e a versão de desenvolvimento "pacote Web Scraping do TCM":
Fonte: https://www.andrewheiss.com/blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/
_________________________________________________________________________________
```

FROM rocker/tidyverse
LABEL maintainer="Observatorio Social de SAJ <observatoriodesaj@gmail.com>"

# Install other libraries
RUN install2.r --error \
        httr plumber janitor git2r zip googledrive \
    && R -e "library(devtools); \
        install_github('pacote_ws_tcm')"


```
_________________________________________________________________________________

Docker API

docker run -d -p 8000:8000 --name=api_tcm_despesas -v /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api:/home/api -v /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/dados_exportados:/home/dados_exportados trestletech/plumber /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api/api_csv.R

docker exec -d api_tcm_despesas sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/dados_exportados
docker exec -d api_tcm_despesas sudo chmod -R 770 /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/dados_exportados

docker exec -d api_tcm_despesas sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api
docker exec -d api_tcm_despesas sudo chmod -R 770 /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api/api



sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api
sudo chmod -R 770 /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api


Vejamos a baixo alguns dos parâmetros que podemos utilizar com ele:

-i permite interagir com o container
-t associa o seu terminal ao terminal do container
-it é apenas uma forma reduzida de escrever -i -t
--name algum-nome permite atribuir um nome ao container em execução
-p 8080:80 mapeia a porta 80 do container para a porta 8080 do host
-d executa o container em background
-v /pasta/host:/pasta/container cria um volume '/pasta/container' dentro do container com o conteúdo da pasta '/pasta/host' do host


- Área que indica os BUGs a serem corrigidos, e as Melhorias, Implementações e Testes que precisam ser feitas no Código.


```{r eval=FALSE, include=FALSE}

######################################################################################

# PRIORIDADES
#!!! Implementar uma rotina para evitar timeout nos Webs Scrapings que estão
#    contidos nas funções 'criar_tb_dmunicipios_entidades' e 'criar_tb_dmunicipios'

#!!! Desenvolver o Packrat do Web Scraping;
#!!! Implantar o Docker do RStudio com o Packrat do Web Scraping;
#!!! Criar usuários com mesmas permissões e acesso a pastas

#!!! Definir a melhor forma de encademaneto na execução das funções primárias para
#    iniciar e continuar o Web Scraping. Além disso, estabelecer rotinas de check no momento de 
#    fazer o input do anos_alvos e cod_municipio
#!!! Finalizar o Código para que se torne automatizado com o CronR/CronTab
#    http://leg.ufpr.br/~walmes/ensino/web-scraping/li%C3%A7%C3%B5es/13-crontab/

#!!! Documentar todo o código;
#!!! Desenvolver um Pacote R para o Web Scraping do TCM-Ba. Incluir uma tabela com o código e nome
#    dos municípios e entidades municipais

#!!! Implementar um API com o pacote 'Plumber';


-------------------------------------------------------------------------------------

# BUG:
#!!! Error: Missing data cannot be written (esse erro está aparecendo depois de milhares requisições)
#Error: Missing data cannot be written 
#In addition: Warning message:
#call dbDisconnect() when finished working with a connection 
#= É um erro provocado por uma função do pacote xml2 (função 'write_html.xml_missing').... Verificar também se esse erro não está ligado à condicional NOT NULL na tabela de requisições


# [1] "Web Scraping das páginas_links das entidades alvos finalizado!"
# Warning message:
# In dir.create(file.path("resposta_scraping_links", nm_municipio,  :
#   'resposta_scraping_links/ITABUNA/EMPRESA MUNICIPAL DE AGUA E SANEAMENTO SA ITABUNA' already exists

#---------------------
# [1] "Scraping - Ano: 2016 - Pág: 16 - Doc: 364 - Valor: 478.4 - CAMARA MUNICIPAL DE SANTO AMARO"
# [1] "Scraping - Ano: 2016 - Pág: 16 - Doc: 365 - Valor: 2000.0 - CAMARA MUNICIPAL DE SANTO AMARO"
# Error: Missing data cannot be written
#----------------------
# #### Erro: 'Timeout' da Primeira tentativa para: 2013-347-pag_196_.html - doc: 01891-13 ####
# #### Iniciando Segunda tentativa para: 2013-347-pag_196_.html - doc: 01891-13 ...
# Error: Missing data cannot be written

# [1] "Scraping - Ano: 2018 - Pág: 477 - Doc: 5042 - Valor: 50.96 - PREFEITURA MUNICIPAL DE PAULO AFONSO"
# [1] "Scraping - Ano: 2018 - Pág: 477 - Doc: 5039 - Valor: 20607.87 - PREFEITURA MUNICIPAL DE PAULO AFONSO"
# [1] "Scraping - Ano: 2018 - Pág: 477 - Doc: 5047 - Valor: 1141.14 - PREFEITURA MUNICIPAL DE PAULO AFONSO"
# Error: Missing data cannot be written

# [1] "Scraping - Ano: 2018 - Pág: 394 - Doc: 00554-18-FUNDEB - Valor: 515759.43 - PREFEITURA MUNICIPAL DE EUNAPOLIS"
# [1] "Scraping - Ano: 2018 - Pág: 1690 - Doc: 30792018 - Valor: 6195.2 - PREFEITURA MUNICIPAL DE SALVADOR"
# #### Erro: 'Timeout' da Primeira tentativa para: 2018-334-pag_1690_.html - doc: 17282018 ####
# #### Iniciando Segunda tentativa para: 2018-334-pag_1690_.html - doc: 17282018 ...
# Error: Missing data cannot be written

# [1] "Parser: 2018-535-pag_12_.html - CAMARA MUNICIPAL DE DOM MACEDO COSTA"
# [1] "Tabela de requisições criada com sucesso"
# Warning message:
# call dbDisconnect() when finished working with a connection 




# In addition: Warning message:
# call dbDisconnect() when finished working with a connection 
#----------------------
# Warning message:
# In dir.create(file.path("resposta_scraping_links", nm_municipio,  :
#   'resposta_scraping_links/ITABUNA/EMPRESA MUNICIPAL DE AGUA E SANEAMENTO SA ITABUNA' already exists
#----------------------



# MELHORIAS
#!!! Criar 3 Banco de Dados. OLTP - Para o Web Scraping; Data Stage Area - Para os Dados Extraídos dos arquivos HMTL de Despesas; 4 - Data WareHouse - Para conectar ao Power BI
#!!! Na função 'executar_csv_googledrive', será preciso armazenar as ID dos arquivos numa tabela após o upload para poder viabilziar o update posteriormente Ou usar alguma outra estratégia ou função do pacote googledrive para nao duplicar os arquivos na pasta;
#!!! Na função 'executar_tidy_data', substituir os "-" que aparecem em 'fonte_de_recurso_tcm' e 'fonte_de_recurso_gestor' pelos valores que estão em "fonte_de_recurso_tcm_2" e 'fonte_de_recurso_gestor_2'
#!!! Procurar uma solução no pacote dplyr ou tibble para o 'armengue' feito na função 'criar_tb_entidades_alvos_paginas' com a função merge.data.frame e verificar se há um método de filtro no dplyr::filter, para que não seja necessário criar a coluna 'filtro'
#!!! Criar um loop na função 'executar_scraping_html_despesas()' para que ele rode o script X vezes até reduzir o número de casos de request com "N"
#!!! Aplicar a função para obter o HASH do HTML somente no nó que coném os dados do web scraping. Não dá para aplicar a todo HTML, pois o cabeçalho muda a cada requisição, alterando sempre o código do hash, ainda que o conteúdo do nó seja igual
#!!! Verificar se é possível separar %in% por any() . Ex: 'any(x == 10)' é muito mais rápido que '10 %in% x' . Se você quiser ver se um vetor contém um único valor
#!!! Retirar a condição 'AUTOINCREMENT' no campo de criação das tabelas, conforme sugerido na documentação no SQLite
#!!! Criar 3 Banco de Dados. OLTP - Para o Web Scraping; Data Stage Area - Para os Dados Extraídos dos arquivos HMTL de Despesas; 4 - Data WareHouse - Para conectar ao Power BI


# IMPLEMENTAÇÕES
# NO CÓDIGO
#!!! Colocar uma condição IF para pausar requisições entre 5h30 a 6h00
#!!! Paralelizar requisições e o tratamento de dados com o pacote `furrr` ou com a função abjutils::pvec() ;

#!!! Implementar um API com o pacote 'Plumber';
#!!! Gerar 'alertas' sobre os pagamentos para publicar no Facebook, Telegram ou Facebook
#    Ex: Despesas de festas em perídos não festivos; Ou emitir relatório resumido na forma de imagem
#    para ser enviado via Telegram, após obter os dados completos do mês;
#!!! Integrar o R com o Telegram
# https://blog.datascienceheroes.com/get-notify-when-an-r-script-finishes-on-telegram/

#!!! Adequar o código para aceitar conexão com o MySQL, PostgreSQL e MongoDB;
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# NA INFRAESTRUTURA
#...


# FALTA TESTAR
#!!! A abordagem para evitar a criação de links de requisição já existeste na 'tabela_requisicoes'
#    ao tratar páginas de despesas que deixaram de ter menos de 20 links, após a complementação
#    dos dados pelo ente municipal. Realizei um commit em relação a essa etapa;


######################################################################################

```
