---
title: "Web Scraping - Despesas dos Municípios do Estado da Bahia, via site do Tribunal de Contas dos Municípios do Estado da Bahia - TCM-Ba"
author: "George Santiago"
date: "18 de julho de 2018"
output: html_document
by: keepgrabbing.py - In memory of Aaron Swartz
---

# Sobre a proposta e objetivo do Web Scraping


# Sobre o Código do Web Scraping em Linguagem R


## Etapas e Estratégias do Web Scraping



## Estrutura do Código em Linguagem R do Web Scraping


### Código do Web Scraping em Linguagem R 
- Carregar os pacotes utilizados no script

Nessa etapa, iremos carregar os pacotes utilizadas em todo o Web Scraping, bem como definir o diretório de trabalho, que é de fundamental importância na etapa de criação das pastas para armazenamento dos dados obtidos. Se os pacotes não estivem instalados na máquina, será necessário utilizar o comando *install.packages("nome_do_pacote")* para que consiga ser carregado com na função *library()*



```{r executar, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE, output=FALSE}


# Variável que define a partir de que ano os dados serão raspados.
anos_alvos <- c("2018")

# Variável que define os municípios (e respectios entes municipais) serão raspados.
# Web Scraping das entidades municipais somente do Município de Salvador (Prefeitura, Câmara e Adm. Indireta)
#cod_municipios <- c(2928703)

# Web Scraping das entidades municipais dos Municípios de:
# Santo Antõnio de Jesus, Santo Amaro, Nazaré, Porto Seguro, Ilhéus, Barreiras, São Francisco do Conde, Salvador e Jequié
# cod_municipios <- c(2928703, 2928604, 2922508, 2925303, 2913606, 2903201, 2929206, 2927408, 2918001)

# Santo Antõnio de Jesus, Santo Amaro, Nazaré, Dom Macedo Costa, Porto Seguro, Ilhéus, Barreiras, São Francisco do Conde, Jequié, Catu, Valença e Lauro de Freitas, Irecê, Ipiaú, Feira de Santana, Salvador
 cod_municipios <- c(2928703, 2928604, 2910206, 2922508, 2925303, 2913606, 2903201, 2929206, 2918001, 2907509, 2919207, 2918407, 2933307, 2905701, 2914802, 2931350, 2900702, 2930709, 2924009, 2910727, 2932903, 2906501, 2911709)


```


# Docker

------------------ Pré-Configuração do Container Docker do Rstudio na Digital Ocean ------------------
_________________________________________________________________________________
```

#cloud-config
runcmd:

docker run -d -t -p 8787:8787 --name=web_scraping_ossaj -e ROOT=TRUE -e PASSWORD=senhadoUsuario -v /home/rstudio/os_saj_web_scraping:/home/rstudio/os_saj_web_scraping rocker/tidyverse
  
sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping

sudo chmod -R 770 /home/rstudio/os_saj_web_scraping

adduser ws_despesa

adduser ws_pessoal


sudo apt-get update

sudo apt-get install -y cron

sudo cron start

```
_________________________________________________________________________________


------------------ Configuração do Container Docker do Rstudio no Shell do Linux ------------------

- Levantar um container com pasta Volume (3 etapas)
```

docker run -d -t -p 8787:8787 --name=web_scraping_ossaj -e ROOT=TRUE -e PASSWORD=senhadoUsuario -v /home/rstudio/os_saj_web_scraping:/home/rstudio/os_saj_web_scraping rocker/tidyverse

docker exec -d web_scraping_ossaj sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping

docker exec -d web_scraping_ossaj sudo chmod -R 770 /home/rstudio/os_saj_web_scraping


docker exec -it web_scraping_ossaj bash

adduser ws_despesa

adduser ws_pessoal


```

------------------------------------ Docker File Personalizado ------------------------------------


Exemplo Super básica fácil: Este é rocker/tidyverse com as versões CRAN httr, plumber, git2r, janitor, zip e googledrive e a versão de desenvolvimento "pacote Web Scraping do TCM":
Fonte: https://www.andrewheiss.com/blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/
_________________________________________________________________________________
```

FROM rocker/tidyverse
LABEL maintainer="Observatorio Social de SAJ <observatoriodesaj@gmail.com>"

# Install other libraries
RUN install2.r --error \
        httr plumber janitor git2r zip googledrive \
    && R -e "library(devtools); \
        install_github('pacote_ws_tcm')"


```
_________________________________________________________________________________

Docker API

docker run -d -p 8000:8000 --name=api_tcm_despesas -v /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api:/home/api -v /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/dados_exportados:/home/dados_exportados trestletech/plumber /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api/api_csv.R

docker exec -d api_tcm_despesas sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/dados_exportados
docker exec -d api_tcm_despesas sudo chmod -R 770 /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/dados_exportados

docker exec -d api_tcm_despesas sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api
docker exec -d api_tcm_despesas sudo chmod -R 770 /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api/api



sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api
sudo chmod -R 770 /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api


Vejamos a baixo alguns dos parâmetros que podemos utilizar com ele:

-i permite interagir com o container
-t associa o seu terminal ao terminal do container
-it é apenas uma forma reduzida de escrever -i -t
--name algum-nome permite atribuir um nome ao container em execução
-p 8080:80 mapeia a porta 80 do container para a porta 8080 do host
-d executa o container em background
-v /pasta/host:/pasta/container cria um volume '/pasta/container' dentro do container com o conteúdo da pasta '/pasta/host' do host


- Área que indica os BUGs a serem corrigidos, e as Melhorias, Implementações e Testes que precisam ser feitas no Código.


```{r eval=FALSE, include=FALSE}

######################################################################################

# PRIORIDADES
#!!! Documentar todo o código;

#!!! Implementar uma rotina para evitar timeout nos Webs Scrapings que estão
#    contidos nas funções 'criar_tb_dmunicipios_entidades' e 'criar_tb_dmunicipios'

#!!! Implementar um ChatBot pelo Telegram com o pacote 'Plumber';


-------------------------------------------------------------------------------------

# BUG:
#!!! Error: Missing data cannot be written (esse erro está aparecendo depois de milhares requisições)
#Error: Missing data cannot be written 
#In addition: Warning message:
#call dbDisconnect() when finished working with a connection 
#= É um erro provocado por uma função do pacote xml2 (função 'write_html.xml_missing').... Verificar também se esse erro não está ligado à condicional NOT NULL na tabela de requisições
# [1] "Scraping - Ano: 2016 - Pág: 16 - Doc: 364 - Valor: 478.4 - CAMARA MUNICIPAL DE SANTO AMARO"
# [1] "Scraping - Ano: 2016 - Pág: 16 - Doc: 365 - Valor: 2000.0 - CAMARA MUNICIPAL DE SANTO AMARO"
# Error: Missing data cannot be written

# !!! Erro intermitente:
# Error in (function (classes, fdef, mtable)  : 
# unable to find an inherited method for function ‘dbWriteTable’ for signature ‘"SQLiteConnection", "character"



# MELHORIAS
#!!! Carregar 'tb_pag_links <- DBI::dbReadTable(connect_sgbd(sgbd), "tabela_paginas_links")' antes do loop com o purrr

#!!! Desenvolver uma nova lógica para a função "executar_scraping_num_pags"

#!!! Usar a função 'fs::path_file_sanitize()' para sanear caminhos de arquivos

# !!! Verificar a necessidade de realizar essa troca
# tb_municipios_alvos_novos <- tidyr::crossing(tb_dcalendario,
#                                              tb_municipios_alvos_novos) %>%
# tb_municipios_alvos_novos <- merge.data.frame(tb_dcalendario,
#                                              tb_municipios_alvos_novos) %>%

#!!! Criar 3 Banco de Dados. OLTP - Para o Web Scraping; Data Stage Area - Para os Dados Extraídos dos arquivos HMTL de Despesas; 4 - Data WareHouse - Para conectar ao Power BI

#!!! Procurar uma solução no pacote dplyr ou tibble para o 'armengue' feito na função 'criar_tb_entidades_alvos_paginas' com a função merge.data.frame e verificar se há um método de filtro no dplyr::filter, para que não seja necessário criar a coluna 'filtro'

#!!! Retirar a condição 'AUTOINCREMENT' no campo de criação das tabelas, conforme sugerido na documentação no SQLite

#!!! Criar 3 Banco de Dados. OLTP - Para o Web Scraping; Data Stage Area - Para os Dados Extraídos dos arquivos HMTL de Despesas; 4 - Data WareHouse - Para conectar ao Power BI


# IMPLEMENTAÇÕES
# NO CÓDIGO
#!!! Colocar uma condição IF para pausar requisições entre 5h30 a 6h00

#!!! Gerar 'alertas' sobre os pagamentos para publicar no Facebook, Telegram ou Facebook
#    Ex: Despesas de festas em perídos não festivos; Ou emitir relatório resumido na forma de imagem
#    para ser enviado via Telegram, após obter os dados completos do mês;
#!!! Integrar o R com o Telegram
# https://blog.datascienceheroes.com/get-notify-when-an-r-script-finishes-on-telegram/

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# NA INFRAESTRUTURA
#...


# FALTA TESTAR
#!!! A abordagem para evitar a criação de links de requisição já existeste na 'tabela_requisicoes'
#    ao tratar páginas de despesas que deixaram de ter menos de 20 links, após a complementação
#    dos dados pelo ente municipal. Realizei um commit em relação a essa etapa;


######################################################################################

```

