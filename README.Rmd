---
title: "Web Scraping - Despesas dos Municípios do Estado da Bahia, via site do Tribunal de Contas dos Municípios do Estado da Bahia - TCM-Ba"
author: "George Santiago"
date: "18 de julho de 2018"
output: html_document
by: keepgrabbing.py - In memory of Aaron Swartz
---

# Sobre a proposta e objetivo do Web Scraping


# Sobre o Código do Web Scraping em Linguagem R


## Etapas e Estratégias do Web Scraping



## Estrutura do Código em Linguagem R do Web Scraping


### Código do Web Scraping em Linguagem R 
- Carregar os pacotes utilizados no script

Nessa etapa, iremos carregar os pacotes utilizadas em todo o Web Scraping, bem como definir o diretório de trabalho, que é de fundamental importância na etapa de criação das pastas para armazenamento dos dados obtidos. Se os pacotes não estivem instalados na máquina, será necessário utilizar o comando *install.packages("nome_do_pacote")* para que consiga ser carregado com na função *library()*



```{r executar, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE, output=FALSE}


# Variável que define a partir de que ano os dados serão raspados.
anos_alvos <- c("2018")

# Variável que define os municípios (e respectios entes municipais) serão raspados.
# Web Scraping das entidades municipais somente do Município de Salvador (Prefeitura, Câmara e Adm. Indireta)
#cod_municipios <- c(2928703)

# Web Scraping das entidades municipais dos Municípios de:
# Santo Antõnio de Jesus, Santo Amaro, Nazaré, Porto Seguro, Ilhéus, Barreiras, São Francisco do Conde, Salvador e Jequié
# cod_municipios <- c(2928703, 2928604, 2922508, 2925303, 2913606, 2903201, 2929206, 2927408, 2918001)

# Santo Antõnio de Jesus, Santo Amaro, Nazaré, Dom Macedo Costa, Porto Seguro, Ilhéus, Barreiras, São Francisco do Conde, Jequié, Catu, Valença e Lauro de Freitas, Irecê, Ipiaú, Feira de Santana, Salvador
 cod_municipios <- c(2928703, 2928604, 2910206, 2922508, 2925303, 2913606, 2903201, 2929206, 2918001, 2907509, 2919207, 2918407, 2933307, 2905701, 2914802, 2931350, 2900702, 2930709, 2924009, 2910727, 2932903, 2906501, 2911709)


```


# Docker

------------------ Pré-Configuração do Container Docker do Rstudio na Digital Ocean ------------------
_________________________________________________________________________________
```

#cloud-config
runcmd:

docker run -d -t -p 8787:8787 --name=web_scraping_ossaj -e ROOT=TRUE -e PASSWORD=senhadoUsuario -v /home/rstudio/os_saj_web_scraping:/home/rstudio/os_saj_web_scraping rocker/tidyverse
  
sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping

sudo chmod -R 770 /home/rstudio/os_saj_web_scraping

adduser ws_despesa

adduser ws_pessoal


sudo apt-get update

sudo apt-get install -y cron

sudo cron start

```
_________________________________________________________________________________


------------------ Configuração do Container Docker do Rstudio no Shell do Linux ------------------

- Levantar um container com pasta Volume (3 etapas)
```

docker run -d -t -p 8787:8787 --name=web_scraping_ossaj -e ROOT=TRUE -e PASSWORD=senhadoUsuario -v /home/rstudio/os_saj_web_scraping:/home/rstudio/os_saj_web_scraping rocker/tidyverse

docker exec -d web_scraping_ossaj sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping

docker exec -d web_scraping_ossaj sudo chmod -R 770 /home/rstudio/os_saj_web_scraping


docker exec -it web_scraping_ossaj bash

adduser ws_despesa

adduser ws_pessoal


```

------------------------------------ Docker File Personalizado ------------------------------------


Exemplo Super básica fácil: Este é rocker/tidyverse com as versões CRAN httr, plumber, git2r, janitor, zip e googledrive e a versão de desenvolvimento "pacote Web Scraping do TCM":
Fonte: https://www.andrewheiss.com/blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/
_________________________________________________________________________________
```

FROM rocker/tidyverse
LABEL maintainer="Observatorio Social de SAJ <observatoriodesaj@gmail.com>"

# Install other libraries
RUN install2.r --error \
        httr plumber janitor git2r zip googledrive \
    && R -e "library(devtools); \
        install_github('pacote_ws_tcm')"


```
_________________________________________________________________________________

Docker API

docker run -d -p 8000:8000 --name=api_tcm_despesas -v /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api:/home/api -v /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/dados_exportados:/home/dados_exportados trestletech/plumber /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api/api_csv.R

docker exec -d api_tcm_despesas sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/dados_exportados
docker exec -d api_tcm_despesas sudo chmod -R 770 /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/dados_exportados

docker exec -d api_tcm_despesas sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api
docker exec -d api_tcm_despesas sudo chmod -R 770 /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api/api



sudo chgrp -R rstudio /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api
sudo chmod -R 770 /home/rstudio/os_saj_web_scraping/tcm_despesas_municipais/api


Vejamos a baixo alguns dos parâmetros que podemos utilizar com ele:

-i permite interagir com o container
-t associa o seu terminal ao terminal do container
-it é apenas uma forma reduzida de escrever -i -t
--name algum-nome permite atribuir um nome ao container em execução
-p 8080:80 mapeia a porta 80 do container para a porta 8080 do host
-d executa o container em background
-v /pasta/host:/pasta/container cria um volume '/pasta/container' dentro do container com o conteúdo da pasta '/pasta/host' do host


- Área que indica os BUGs a serem corrigidos, e as Melhorias, Implementações e Testes que precisam ser feitas no Código.


```{r eval=FALSE, include=FALSE}

######################################################################################

# PRIORIDADES
#!!! Implementar uma rotina para evitar timeout nos Webs Scrapings que estão
#    contidos nas funções 'criar_tb_dmunicipios_entidades' e 'criar_tb_dmunicipios'

#!!! Desenvolver o Packrat do Web Scraping;
#!!! Implantar o Docker do RStudio com o Packrat do Web Scraping;
#!!! Criar usuários com mesmas permissões e acesso a pastas

#!!! Definir a melhor forma de encademaneto na execução das funções primárias para
#    iniciar e continuar o Web Scraping. Além disso, estabelecer rotinas de check no momento de 
#    fazer o input do anos_alvos e cod_municipio
#!!! Finalizar o Código para que se torne automatizado com o CronR/CronTab
#    http://leg.ufpr.br/~walmes/ensino/web-scraping/li%C3%A7%C3%B5es/13-crontab/

#!!! Documentar todo o código;
#!!! Desenvolver um Pacote R para o Web Scraping do TCM-Ba. Incluir uma tabela com o código e nome
#    dos municípios e entidades municipais

#!!! Implementar um API com o pacote 'Plumber';


-------------------------------------------------------------------------------------

# BUG:
#!!! Error: Missing data cannot be written (esse erro está aparecendo depois de milhares requisições)
#Error: Missing data cannot be written 
#In addition: Warning message:
#call dbDisconnect() when finished working with a connection 
#= É um erro provocado por uma função do pacote xml2 (função 'write_html.xml_missing').... Verificar também se esse erro não está ligado à condicional NOT NULL na tabela de requisições


# [1] "Web Scraping das páginas_links das entidades alvos finalizado!"
# Warning message:
# In dir.create(file.path("resposta_scraping_links", nm_municipio,  :
#   'resposta_scraping_links/ITABUNA/EMPRESA MUNICIPAL DE AGUA E SANEAMENTO SA ITABUNA' already exists

#---------------------
# [1] "Scraping - Ano: 2016 - Pág: 16 - Doc: 364 - Valor: 478.4 - CAMARA MUNICIPAL DE SANTO AMARO"
# [1] "Scraping - Ano: 2016 - Pág: 16 - Doc: 365 - Valor: 2000.0 - CAMARA MUNICIPAL DE SANTO AMARO"
# Error: Missing data cannot be written
#----------------------
# #### Erro: 'Timeout' da Primeira tentativa para: 2013-347-pag_196_.html - doc: 01891-13 ####
# #### Iniciando Segunda tentativa para: 2013-347-pag_196_.html - doc: 01891-13 ...
# Error: Missing data cannot be written

# [1] "Scraping - Ano: 2018 - Pág: 477 - Doc: 5042 - Valor: 50.96 - PREFEITURA MUNICIPAL DE PAULO AFONSO"
# [1] "Scraping - Ano: 2018 - Pág: 477 - Doc: 5039 - Valor: 20607.87 - PREFEITURA MUNICIPAL DE PAULO AFONSO"
# [1] "Scraping - Ano: 2018 - Pág: 477 - Doc: 5047 - Valor: 1141.14 - PREFEITURA MUNICIPAL DE PAULO AFONSO"
# Error: Missing data cannot be written

# [1] "Scraping - Ano: 2018 - Pág: 394 - Doc: 00554-18-FUNDEB - Valor: 515759.43 - PREFEITURA MUNICIPAL DE EUNAPOLIS"
# [1] "Scraping - Ano: 2018 - Pág: 1690 - Doc: 30792018 - Valor: 6195.2 - PREFEITURA MUNICIPAL DE SALVADOR"
# #### Erro: 'Timeout' da Primeira tentativa para: 2018-334-pag_1690_.html - doc: 17282018 ####
# #### Iniciando Segunda tentativa para: 2018-334-pag_1690_.html - doc: 17282018 ...
# Error: Missing data cannot be written

# [1] "Parser: 2018-535-pag_12_.html - CAMARA MUNICIPAL DE DOM MACEDO COSTA"
# [1] "Tabela de requisições criada com sucesso"
# Warning message:
# call dbDisconnect() when finished working with a connection 




# In addition: Warning message:
# call dbDisconnect() when finished working with a connection 
#----------------------
# Warning message:
# In dir.create(file.path("resposta_scraping_links", nm_municipio,  :
#   'resposta_scraping_links/ITABUNA/EMPRESA MUNICIPAL DE AGUA E SANEAMENTO SA ITABUNA' already exists
#----------------------



# MELHORIAS
#!!! Criar 3 Banco de Dados. OLTP - Para o Web Scraping; Data Stage Area - Para os Dados Extraídos dos arquivos HMTL de Despesas; 4 - Data WareHouse - Para conectar ao Power BI
#!!! Na função 'executar_csv_googledrive', será preciso armazenar as ID dos arquivos numa tabela após o upload para poder viabilziar o update posteriormente Ou usar alguma outra estratégia ou função do pacote googledrive para nao duplicar os arquivos na pasta;
#!!! Na função 'executar_tidy_data', substituir os "-" que aparecem em 'fonte_de_recurso_tcm' e 'fonte_de_recurso_gestor' pelos valores que estão em "fonte_de_recurso_tcm_2" e 'fonte_de_recurso_gestor_2'
#!!! Procurar uma solução no pacote dplyr ou tibble para o 'armengue' feito na função 'criar_tb_entidades_alvos_paginas' com a função merge.data.frame e verificar se há um método de filtro no dplyr::filter, para que não seja necessário criar a coluna 'filtro'
#!!! Criar um loop na função 'executar_scraping_html_despesas()' para que ele rode o script X vezes até reduzir o número de casos de request com "N"
#!!! Aplicar a função para obter o HASH do HTML somente no nó que coném os dados do web scraping. Não dá para aplicar a todo HTML, pois o cabeçalho muda a cada requisição, alterando sempre o código do hash, ainda que o conteúdo do nó seja igual
#!!! Verificar se é possível separar %in% por any() . Ex: 'any(x == 10)' é muito mais rápido que '10 %in% x' . Se você quiser ver se um vetor contém um único valor
#!!! Retirar a condição 'AUTOINCREMENT' no campo de criação das tabelas, conforme sugerido na documentação no SQLite
#!!! Criar 3 Banco de Dados. OLTP - Para o Web Scraping; Data Stage Area - Para os Dados Extraídos dos arquivos HMTL de Despesas; 4 - Data WareHouse - Para conectar ao Power BI


# IMPLEMENTAÇÕES
# NO CÓDIGO
#!!! Colocar uma condição IF para pausar requisições entre 5h30 a 6h00
#!!! Paralelizar requisições e o tratamento de dados com o pacote `furrr` ou com a função abjutils::pvec() ;

#!!! Implementar um API com o pacote 'Plumber';
#!!! Gerar 'alertas' sobre os pagamentos para publicar no Facebook, Telegram ou Facebook
#    Ex: Despesas de festas em perídos não festivos; Ou emitir relatório resumido na forma de imagem
#    para ser enviado via Telegram, após obter os dados completos do mês;
#!!! Integrar o R com o Telegram
# https://blog.datascienceheroes.com/get-notify-when-an-r-script-finishes-on-telegram/

#!!! Adequar o código para aceitar conexão com o MySQL, PostgreSQL e MongoDB;
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# NA INFRAESTRUTURA
#...


# FALTA TESTAR
#!!! A abordagem para evitar a criação de links de requisição já existeste na 'tabela_requisicoes'
#    ao tratar páginas de despesas que deixaram de ter menos de 20 links, após a complementação
#    dos dados pelo ente municipal. Realizei um commit em relação a essa etapa;


######################################################################################

```

